{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#For text processing\n",
    "#Using this (https://bit.ly/2HvV2dx) as template\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer #, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chtra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chtra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty</th>\n",
       "      <th>distance</th>\n",
       "      <th>elevation</th>\n",
       "      <th>name</th>\n",
       "      <th>nreviews</th>\n",
       "      <th>route_type</th>\n",
       "      <th>stars</th>\n",
       "      <th>trail_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EASY</td>\n",
       "      <td>15.9 km</td>\n",
       "      <td>149 m</td>\n",
       "      <td>Highland Creek Trail</td>\n",
       "      <td>33</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.4</td>\n",
       "      <td>[dogs on leash, hiking, mountain biking, road ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EASY</td>\n",
       "      <td>5 km</td>\n",
       "      <td>14 m</td>\n",
       "      <td>Unionville Valleylands Trail</td>\n",
       "      <td>29</td>\n",
       "      <td>Loop</td>\n",
       "      <td>3.9</td>\n",
       "      <td>[dogs on leash, wheelchair friendly, kid frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EASY</td>\n",
       "      <td>15.6 km</td>\n",
       "      <td>166 m</td>\n",
       "      <td>Greenwood Conservation Area Trail</td>\n",
       "      <td>31</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.1</td>\n",
       "      <td>[dogs on leash, kid friendly, birding, hiking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>3.4 km</td>\n",
       "      <td>23 m</td>\n",
       "      <td>Woodland Trail</td>\n",
       "      <td>22</td>\n",
       "      <td>Loop</td>\n",
       "      <td>3.9</td>\n",
       "      <td>[dogs on leash, kid friendly, hiking, trail ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>4.7 km</td>\n",
       "      <td>64 m</td>\n",
       "      <td>Hilton Falls Trail</td>\n",
       "      <td>238</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.3</td>\n",
       "      <td>[dogs on leash, birding, cross country skiing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>2.3 km</td>\n",
       "      <td>100 m</td>\n",
       "      <td>Niagara Glen Trail</td>\n",
       "      <td>135</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.7</td>\n",
       "      <td>[dogs on leash, kid friendly, hiking, nature t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>7.2 km</td>\n",
       "      <td>128 m</td>\n",
       "      <td>Nassagaweya and Bruce Trail Loop from Rattlesn...</td>\n",
       "      <td>170</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.2</td>\n",
       "      <td>[dogs on leash, kid friendly, hiking, nature t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>15.3 km</td>\n",
       "      <td>427 m</td>\n",
       "      <td>Lion's Head Loop Via Bruce Trail</td>\n",
       "      <td>117</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.8</td>\n",
       "      <td>[dogs on leash, birding, hiking, nature trips,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>2.9 km</td>\n",
       "      <td>71 m</td>\n",
       "      <td>Dundas Peak via Bruce Trail and BT Side Trail</td>\n",
       "      <td>82</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.1</td>\n",
       "      <td>[dogs on leash, birding, hiking, road biking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>8 km</td>\n",
       "      <td>222 m</td>\n",
       "      <td>Forks of the Credit and Cataract Falls via Bru...</td>\n",
       "      <td>124</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.4</td>\n",
       "      <td>[dogs on leash, birding, hiking, nature trips,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  difficulty distance elevation  \\\n",
       "0       EASY  15.9 km     149 m   \n",
       "1       EASY     5 km      14 m   \n",
       "2       EASY  15.6 km     166 m   \n",
       "3   MODERATE   3.4 km      23 m   \n",
       "4   MODERATE   4.7 km      64 m   \n",
       "5   MODERATE   2.3 km     100 m   \n",
       "6   MODERATE   7.2 km     128 m   \n",
       "7   MODERATE  15.3 km     427 m   \n",
       "8   MODERATE   2.9 km      71 m   \n",
       "9   MODERATE     8 km     222 m   \n",
       "\n",
       "                                                name nreviews route_type  \\\n",
       "0                               Highland Creek Trail       33       Loop   \n",
       "1                       Unionville Valleylands Trail       29       Loop   \n",
       "2                  Greenwood Conservation Area Trail       31       Loop   \n",
       "3                                     Woodland Trail       22       Loop   \n",
       "4                                 Hilton Falls Trail      238       Loop   \n",
       "5                                 Niagara Glen Trail      135       Loop   \n",
       "6  Nassagaweya and Bruce Trail Loop from Rattlesn...      170       Loop   \n",
       "7                  Lion's Head Loop Via Bruce Trail       117       Loop   \n",
       "8      Dundas Peak via Bruce Trail and BT Side Trail       82       Loop   \n",
       "9  Forks of the Credit and Cataract Falls via Bru...      124       Loop   \n",
       "\n",
       "  stars                                   trail_attributes  \n",
       "0   4.4  [dogs on leash, hiking, mountain biking, road ...  \n",
       "1   3.9  [dogs on leash, wheelchair friendly, kid frien...  \n",
       "2   4.1  [dogs on leash, kid friendly, birding, hiking,...  \n",
       "3   3.9  [dogs on leash, kid friendly, hiking, trail ru...  \n",
       "4   4.3  [dogs on leash, birding, cross country skiing,...  \n",
       "5   4.7  [dogs on leash, kid friendly, hiking, nature t...  \n",
       "6   4.2  [dogs on leash, kid friendly, hiking, nature t...  \n",
       "7   4.8  [dogs on leash, birding, hiking, nature trips,...  \n",
       "8   4.1  [dogs on leash, birding, hiking, road biking, ...  \n",
       "9   4.4  [dogs on leash, birding, hiking, nature trips,...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "df=pd.read_pickle('hiking_attributes_ontario.pkl')\n",
    "print (len(df))\n",
    "# df['review'][0]\n",
    "df.head(50)\n",
    "\n",
    "df_work=df.copy()\n",
    "df_work.head(10)\n",
    "# df_work.route_type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.0\n"
     ]
    }
   ],
   "source": [
    "#Weighted average of ratings\n",
    "#Mean rating across all hikes\n",
    "# df_work['stars'].mean()\n",
    "ratings=pd.to_numeric(df_work.stars)\n",
    "ratings.mean()\n",
    "\n",
    "#Min # of ratings to be included\n",
    "nreviews=pd.to_numeric(df_work.nreviews)\n",
    "m=nreviews.quantile(0.8)\n",
    "print (m)\n",
    "# print(nreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e2ea393400>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEYdJREFUeJzt3X+sX3V9x/Hna4DMUMIPkZtamhRnZ0SJCDeMxMXcyiI//KOYDAMhWpSl/oFGM5as6h+yOBJcpmRmjKQOYv0xr0QkNICbrOOGmAyRMuRXw6jaQWlDw0DgasZWfO+Pe6qX8m2/l97zvbft5/lIbr7n+/l+vue8zzunvDjne+73pqqQJLXp9xa7AEnS4jEEJKlhhoAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlqmCEgSQ07crELADjppJNqxYoVQ+f96le/4phjjhl9QQcxe2APwB6APQDYvHnzs1X15vms46AIgRUrVnD//fcPnTc1NcXExMToCzqI2QN7APYA7AFAkv+a7zq8HCRJDTMEJKlhQ0Mgye8nuS/JT5M8muSvuvFTk/w4yRNJvpvkDd340d3zrd3rK0a7C5KkAzWXM4GXgfdX1buBM4Dzk5wDfAm4rqpWAs8DV3TzrwCer6q3Add18yRJB6GhIVAzprunR3U/Bbwf+F43vgG4qFte3T2ne/3cJOmtYklSb+Z0d1CSI4DNwNuA64GfAb+sqt3dlO3Asm55GfAUQFXtTvIC8Cbg2b3WuRZYCzA2NsbU1NTQOqanp+c073BmD+wB2AOwB32ZUwhU1SvAGUmOB24F3jFoWvc46P/6X/Pny6pqPbAeYHx8vOZyq5e3hNkDsAdgD8Ae9OV13R1UVb8EpoBzgOOT7AmRU4Ad3fJ2YDlA9/pxwHN9FCtJ6tdc7g56c3cGQJI3An8CbAHuBv60m7YGuK1b3tg9p3v938o/ZCxJB6W5XA5aCmzoPhf4PeDmqro9yWPAZJK/Bv4DuLGbfyPwzSRbmTkDuGQEdf/WinV3jHL1+7Xt2g8u2rYlqQ9DQ6CqHgLeM2D858DZA8b/B7i4l+okSSPlbwxLUsMMAUlqmCEgSQ0zBCSpYYaAJDXMEJCkhhkCktQwQ0CSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUMENAkhpmCEhSwwwBSWqYISBJDTMEJKlhhoAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlq2NAQSLI8yd1JtiR5NMmnu/Grkzyd5MHu58JZ7/lskq1JHk9y3ih3QJJ04I6cw5zdwFVV9UCSY4HNSe7qXruuqv529uQkpwGXAO8E3gL8a5I/rKpX+ixckjR/Q88EqmpnVT3QLb8EbAGW7ectq4HJqnq5qn4BbAXO7qNYSVK/UlVzn5ysAO4B3gX8OXA58CJwPzNnC88n+Xvg3qr6VveeG4EfVNX39lrXWmAtwNjY2FmTk5NDtz89Pc2SJUteNfbw0y/Muf6+nb7suAXf5qAetMYe2AOwBwCrVq3aXFXj81nHXC4HAZBkCXAL8JmqejHJDcAXgeoevwx8HMiAt78maapqPbAeYHx8vCYmJobWMDU1xd7zLl93x1x3oXfbLptY8G0O6kFr7IE9AHvQlzndHZTkKGYC4NtV9X2Aqnqmql6pqt8AX+N3l3y2A8tnvf0UYEd/JUuS+jKXu4MC3AhsqaqvzBpfOmvah4BHuuWNwCVJjk5yKrASuK+/kiVJfZnL5aD3Ah8BHk7yYDf2OeDSJGcwc6lnG/AJgKp6NMnNwGPM3Fl0pXcGSdLBaWgIVNWPGHyd/879vOca4Jp51CVJWgD+xrAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlqmCEgSQ0zBCSpYYaAJDXMEJCkhhkCktQwQ0CSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUMENAkhpmCEhSwwwBSWqYISBJDTMEJKlhhoAkNcwQkKSGDQ2BJMuT3J1kS5JHk3y6Gz8xyV1JnugeT+jGk+SrSbYmeSjJmaPeCUnSgZnLmcBu4KqqegdwDnBlktOAdcCmqloJbOqeA1wArOx+1gI39F61JKkXQ0OgqnZW1QPd8kvAFmAZsBrY0E3bAFzULa8GvlEz7gWOT7K098olSfOWqpr75GQFcA/wLuDJqjp+1mvPV9UJSW4Hrq2qH3Xjm4C/rKr791rXWmbOFBgbGztrcnJy6Panp6dZsmTJq8YefvqFOdfft9OXHbfg2xzUg9bYA3sA9gBg1apVm6tqfD7rOHKuE5MsAW4BPlNVLybZ59QBY69JmqpaD6wHGB8fr4mJiaE1TE1Nsfe8y9fdMfR9o7LtsokF3+agHrTGHtgDsAd9mdPdQUmOYiYAvl1V3++Gn9lzmad73NWNbweWz3r7KcCOfsqVJPVpLncHBbgR2FJVX5n10kZgTbe8Brht1vhHu7uEzgFeqKqdPdYsSerJXC4HvRf4CPBwkge7sc8B1wI3J7kCeBK4uHvtTuBCYCvwa+BjvVYsSerN0BDoPuDd1wcA5w6YX8CV86xLkrQA/I1hSWqYISBJDTMEJKlhhoAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlqmCEgSQ0zBCSpYYaAJDXMEJCkhhkCktQwQ0CSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUMENAkhpmCEhSwwwBSWqYISBJDRsaAkluSrIrySOzxq5O8nSSB7ufC2e99tkkW5M8nuS8URUuSZq/uZwJfB04f8D4dVV1RvdzJ0CS04BLgHd27/mHJEf0VawkqV9DQ6Cq7gGem+P6VgOTVfVyVf0C2AqcPY/6JEkjNJ/PBD6Z5KHuctEJ3dgy4KlZc7Z3Y5Kkg1CqavikZAVwe1W9q3s+BjwLFPBFYGlVfTzJ9cC/V9W3unk3AndW1S0D1rkWWAswNjZ21uTk5NA6pqenWbJkyavGHn76haHvG5XTlx234Nsc1IPW2AN7APYAYNWqVZuranw+6zjyQN5UVc/sWU7yNeD27ul2YPmsqacAO/axjvXAeoDx8fGamJgYut2pqSn2nnf5ujvmXnjPtl02seDbHNSD1tgDewD2oC8HdDkoydJZTz8E7LlzaCNwSZKjk5wKrATum1+JkqRRGXomkOQ7wARwUpLtwBeAiSRnMHM5aBvwCYCqejTJzcBjwG7gyqp6ZTSlS5Lma2gIVNWlA4Zv3M/8a4Br5lOUJGlh+BvDktQwQ0CSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUMENAkhpmCEhSwwwBSWqYISBJDTMEJKlhhoAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlqmCEgSQ0zBCSpYYaAJDXMEJCkhhkCktQwQ0CSGjY0BJLclGRXkkdmjZ2Y5K4kT3SPJ3TjSfLVJFuTPJTkzFEWL0man7mcCXwdOH+vsXXApqpaCWzqngNcAKzsftYCN/RTpiRpFIaGQFXdAzy31/BqYEO3vAG4aNb4N2rGvcDxSZb2VawkqV8H+pnAWFXtBOgeT+7GlwFPzZq3vRuTJB2Ejux5fRkwVgMnJmuZuWTE2NgYU1NTQ1c+PT39mnlXnb779dbYm7nU3LdBPWiNPbAHYA/6cqAh8EySpVW1s7vcs6sb3w4snzXvFGDHoBVU1XpgPcD4+HhNTEwM3ejU1BR7z7t83R2vt/bebLtsYsG3OagHrbEH9gDsQV8O9HLQRmBNt7wGuG3W+Ee7u4TOAV7Yc9lIknTwGXomkOQ7wARwUpLtwBeAa4Gbk1wBPAlc3E2/E7gQ2Ar8GvjYCGqWJPVkaAhU1aX7eOncAXMLuHK+RUmSFkbfHww3ZcUifB5x1em7mVjwrUo6XPm1EZLUMENAkhpmCEhSwwwBSWqYISBJDTMEJKlhhoAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlqmCEgSQ0zBCSpYYaAJDXMEJCkhhkCktQwQ0CSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zBCQpIYZApLUsCPn8+Yk24CXgFeA3VU1nuRE4LvACmAb8OGqen5+ZUqSRqGPM4FVVXVGVY13z9cBm6pqJbCpey5JOgiN4nLQamBDt7wBuGgE25Ak9WC+IVDAD5NsTrK2Gxurqp0A3ePJ89yGJGlEUlUH/ubkLVW1I8nJwF3Ap4CNVXX8rDnPV9UJA967FlgLMDY2dtbk5OTQ7U1PT7NkyZJXjT389AsHXP+haOyNcPKJxy12GYtq0HHQGntgDwBWrVq1edal+AMyrw+Gq2pH97grya3A2cAzSZZW1c4kS4Fd+3jvemA9wPj4eE1MTAzd3tTUFHvPu3zdHfPZhUPOVafv5sNz6NXhbNBx0Bp7YA/6csCXg5Ick+TYPcvAB4BHgI3Amm7aGuC2+RYpSRqN+ZwJjAG3Jtmznn+qqn9O8hPg5iRXAE8CF8+/TEnSKBxwCFTVz4F3Dxj/b+Dc+RQlSVoY/sawJDXMEJCkhhkCktQwQ0CSGmYISFLDDAFJapghIEkNm9fXRmhxrFjEr8rYdu0HF23bkvrnmYAkNcwQkKSGGQKS1DBDQJIaZghIUsMMAUlqmCEgSQ0zBCSpYYaAJDXMEJCkhhkCktQwQ0CSGmYISFLDDAFJapghIEkNMwQkqWGGgCQ1zL8sptdlsf6qmX/RTBqNkYVAkvOBvwOOAP6xqq4d1bZ0+JsdPledvpvLFzCMDCAdzkZyOSjJEcD1wAXAacClSU4bxbYkSQduVGcCZwNbq+rnAEkmgdXAYyPanjQyXgLT4WxUIbAMeGrW8+3AH41oW5J6tFihB4sXfC3u8x6pqv5XmlwMnFdVf9Y9/whwdlV9atactcDa7unbgcfnsOqTgGd7LvdQYw/sAdgDsAcAb6+qY+ezglGdCWwHls96fgqwY/aEqloPrH89K01yf1WNz7+8Q5c9sAdgD8AewEwP5ruOUf2ewE+AlUlOTfIG4BJg44i2JUk6QCM5E6iq3Uk+CfwLM7eI3lRVj45iW5KkAzey3xOoqjuBO3te7eu6fHSYsgf2AOwB2APooQcj+WBYknRo8LuDJKlhh0wIJDk/yeNJtiZZt9j1LJQk25I8nOTBPXcCJDkxyV1JnugeT1jsOvuU5KYku5I8Mmts4D5nxle74+KhJGcuXuX92UcPrk7ydHcsPJjkwlmvfbbrweNJzlucqvuVZHmSu5NsSfJokk93480cC/vpQX/HQlUd9D/MfLj8M+CtwBuAnwKnLXZdC7Tv24CT9hr7G2Bdt7wO+NJi19nzPr8POBN4ZNg+AxcCPwACnAP8eLHrH2EPrgb+YsDc07p/E0cDp3b/Vo5Y7H3ooQdLgTO75WOB/+z2tZljYT896O1YOFTOBH77NRRV9b/Anq+haNVqYEO3vAG4aBFr6V1V3QM8t9fwvvZ5NfCNmnEvcHySpQtT6ejsowf7shqYrKqXq+oXwFZm/s0c0qpqZ1U90C2/BGxh5tsImjkW9tODfXndx8KhEgKDvoZif404nBTwwySbu9+yBhirqp0wc5AAJy9adQtnX/vc2rHxye5Sx02zLgMe9j1IsgJ4D/BjGj0W9uoB9HQsHCohkAFjrdzW9N6qOpOZb2S9Msn7Frugg0xLx8YNwB8AZwA7gS9344d1D5IsAW4BPlNVL+5v6oCxw6IPA3rQ27FwqITA0K+hOFxV1Y7ucRdwKzOnds/sOc3tHnctXoULZl/73MyxUVXPVNUrVfUb4Gv87jT/sO1BkqOY+Y/ft6vq+91wU8fCoB70eSwcKiHQ5NdQJDkmybF7loEPAI8ws+9rumlrgNsWp8IFta993gh8tLsz5BzghT2XCg43e13f/hAzxwLM9OCSJEcnORVYCdy30PX1LUmAG4EtVfWVWS81cyzsqwe9HguL/en36/iU/EJmPhn/GfD5xa5ngfb5rcx80v9T4NE9+w28CdgEPNE9nrjYtfa8399h5hT3/5j5P5sr9rXPzJz+Xt8dFw8D44td/wh78M1uHx/q/rEvnTX/810PHgcuWOz6e+rBHzNzKeMh4MHu58KWjoX99KC3Y8HfGJakhh0ql4MkSSNgCEhSwwwBSWqYISBJDTMEJKlhhoAkNcwQkKSGGQKS1LD/B66EA2yEw8wJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nreviews.hist()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve hikes with nreviews > m\n",
    "df_work.nreviews=df_work.nreviews.astype(int)\n",
    "# df_filt=df_work.copy().loc(df_work.nreviews>=m)\n",
    "# df_work.nreviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_stemming(text):\n",
    "#     return PorterStemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    words=[word for sentence in sent_tokenize(text) for word in word_tokenize(sentence)]\n",
    "    words=[word for word in words if len(word)>=3]\n",
    "    words=[word for word in words if word not in gensim.parsing.preprocessing.STOPWORDS]\n",
    "    words=[WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "    #Lemmatize\n",
    "#     words=[WordNetLemmatizer.lemmatize(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs on leash', 'hiking', 'mountain biking', 'road biking', 'trail running', 'walking', 'lake', 'partially paved']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "reviews=df_work.trail_attributes\n",
    "#tokenize text\n",
    "# reviews.apply(lambda x: x.lower())\n",
    "# reviews=reviews.str.lower()\n",
    "# reviews=reviews.str.replace('\\\"', '')\n",
    "# reviews=reviews.str.replace(\"\\'\", '')\n",
    "# reviews=reviews.str.replace('\\\"', '')\n",
    "# reviews=reviews.str.replace('\\!', '')\n",
    "# reviews=reviews.str.replace('\\/', ' ')\n",
    "# reviews=reviews.str.replace(',', '')\n",
    "# reviews=reviews.str.replace('(', '')\n",
    "# reviews=reviews.str.replace(')', '')\n",
    "# reviews=reviews.str.replace('.', '')\n",
    "# reviews=reviews.str.replace('\\d+', '')\n",
    "# reviews=reviews.replace(to_replace={'hike', 'walk','ing','run','trail','water','dog', 'interesting', 'easy','good','great','lot','recommend'}, value='', regex=True)\n",
    "print(reviews[0])\n",
    "# processed_text=[preprocess(text) for text in reviews]\n",
    "# lemmatized_text=[WordNetLemmatizer.lemmatize(text) for text in reviews]\n",
    "# WordNetLemmatizer.lemmatize('are')\n",
    "\n",
    "# reviews.head(50)\n",
    "# print (processed_text[0], reviews[0])\n",
    "# print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty</th>\n",
       "      <th>distance</th>\n",
       "      <th>elevation</th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EASY</td>\n",
       "      <td>15.9 km</td>\n",
       "      <td>149 m</td>\n",
       "      <td>Highland Creek Trail</td>\n",
       "      <td>Parts of this trail are very interesting but i...</td>\n",
       "      <td>[part, paved, time, roadway, closed, couple, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EASY</td>\n",
       "      <td>15.9 km</td>\n",
       "      <td>149 m</td>\n",
       "      <td>Highland Creek Trail</td>\n",
       "      <td>good trail as fun for biking</td>\n",
       "      <td>[fun, bik]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EASY</td>\n",
       "      <td>15.9 km</td>\n",
       "      <td>149 m</td>\n",
       "      <td>Highland Creek Trail</td>\n",
       "      <td>A paved trail that takes you through  wooded a...</td>\n",
       "      <td>[paved, take, wooded, area, spent, hour, came,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EASY</td>\n",
       "      <td>15.9 km</td>\n",
       "      <td>149 m</td>\n",
       "      <td>Highland Creek Trail</td>\n",
       "      <td>Started at west entrance of east point park fo...</td>\n",
       "      <td>[started, west, entrance, east, point, park, q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EASY</td>\n",
       "      <td>15.9 km</td>\n",
       "      <td>149 m</td>\n",
       "      <td>Highland Creek Trail</td>\n",
       "      <td>A beautiful, easy trail that traverses beautif...</td>\n",
       "      <td>[beautiful, traverse, beautiful, wooded, area,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  difficulty distance elevation                  name  \\\n",
       "0       EASY  15.9 km     149 m  Highland Creek Trail   \n",
       "1       EASY  15.9 km     149 m  Highland Creek Trail   \n",
       "2       EASY  15.9 km     149 m  Highland Creek Trail   \n",
       "3       EASY  15.9 km     149 m  Highland Creek Trail   \n",
       "4       EASY  15.9 km     149 m  Highland Creek Trail   \n",
       "\n",
       "                                              review  \\\n",
       "0  Parts of this trail are very interesting but i...   \n",
       "1                       good trail as fun for biking   \n",
       "2  A paved trail that takes you through  wooded a...   \n",
       "3  Started at west entrance of east point park fo...   \n",
       "4  A beautiful, easy trail that traverses beautif...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  [part, paved, time, roadway, closed, couple, p...  \n",
       "1                                         [fun, bik]  \n",
       "2  [paved, take, wooded, area, spent, hour, came,...  \n",
       "3  [started, west, entrance, east, point, park, q...  \n",
       "4  [beautiful, traverse, beautiful, wooded, area,...  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add processed text as a new column\n",
    "# df_work['processed_text']= pd.Series(processed_text)\n",
    "# df_work.head()\n",
    "#[' '.join(filter(None,filter(lambda word: word not in stop, line))) for line in data['text_data'].str.lower().str.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part',\n",
       " 'paved',\n",
       " 'time',\n",
       " 'roadway',\n",
       " 'closed',\n",
       " 'couple',\n",
       " 'place',\n",
       " 'forc',\n",
       " 'road',\n",
       " 'probably',\n",
       " 'route',\n",
       " 'bike']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_work.processed_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word countf for IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    " \n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    " \n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"./stop_words.txt\")\n",
    " \n",
    "#get the text column \n",
    "docs=df_work['processed_text'][0] #.tolist()\n",
    " \n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part',\n",
       " 'paved',\n",
       " 'time',\n",
       " 'roadway',\n",
       " 'closed',\n",
       " 'couple',\n",
       " 'place',\n",
       " 'forc',\n",
       " 'road',\n",
       " 'route',\n",
       " 'bike']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(df_work['processed_text'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df_work['processed_text']]\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.036*\"nice\" + 0.014*\"beautiful\" + 0.012*\"area\" + 0.011*\"river\"')\n",
      "(1, '0.032*\"park\" + 0.019*\"nice\" + 0.019*\"winter\" + 0.014*\"route\"')\n",
      "(2, '0.014*\"park\" + 0.012*\"nice\" + 0.012*\"got\" + 0.012*\"spot\"')\n",
      "(3, '0.025*\"fall\" + 0.024*\"beautiful\" + 0.019*\"nice\" + 0.017*\"view\"')\n",
      "(4, '0.022*\"view\" + 0.016*\"lake\" + 0.014*\"beautiful\" + 0.013*\"loop\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-235-9dad6e3f5084>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# run word2vec model and then save it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtexts_stemmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnext_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnext_text\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_work\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'processed_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mw2vmodel_stemmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts_stemmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mw2vmodel_stemmed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msavefolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'w2v_stemmed_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\chtra\\Anaconda3.6\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[1;32mC:\\Users\\chtra\\Anaconda3.6\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\chtra\\Anaconda3.6\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\chtra\\Anaconda3.6\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\chtra\\Anaconda3.6\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\chtra\\Anaconda3.6\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# should be set by `build_vocab`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "# run word2vec model and then save it\n",
    "texts_stemmed = filter(None, [next_text.strip(' ').split(' ') for next_text in df_work['processed_text'][0]])\n",
    "w2vmodel_stemmed = gensim.models.Word2Vec(texts_stemmed, size=100, window=5, min_count=5, workers=4)\n",
    "w2vmodel_stemmed.save(savefolder+'w2v_stemmed_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
