{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#For text processing\n",
    "#Using this (https://bit.ly/2HvV2dx) as template\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer #, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "df=pd.read_pickle('dftst.pkl')\n",
    "print (len(df))\n",
    "# df['review'][0]\n",
    "df.head(50)\n",
    "\n",
    "df_work=df.copy()\n",
    "df_work.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_per_trail=df_work.groupby('name').count()\n",
    "# # reviews_per_trail['review'].hist()\n",
    "# # plt.show()\n",
    "# print(reviews_per_trail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data pre-processing - text cleaning\n",
    "\n",
    "# df_r=df['review']\n",
    "\n",
    "\n",
    "# def preprocess(datframe):\n",
    "#     datframe.apply(lambda x: x.lower())\n",
    "#     datframe.apply(lambda x: x.translate(None, string.punctuation))\n",
    "#     datframe.apply(lambda x: x.translate(None,string.digits))\n",
    "#     return (datframe)\n",
    "\n",
    "# # for i in df_r:\n",
    "# #     print(preprocess(str(i)))\n",
    "    \n",
    "# preprocess(df['review'])\n",
    "# # df_r.head()\n",
    "\n",
    "# # tokens=tokenizer.tokenize(df_r)\n",
    "\n",
    "# # def data_preprocess(dat):\n",
    "# #     tokens=tokenizer.tokenize(df_r)\n",
    "# # #     words=\n",
    "# #     lemmatize=stemmer.stem(WordNetLemmatizer().lemmatize(dat, pos='v'))\n",
    "# #     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_stemming(text):\n",
    "#     return PorterStemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    words=[word for sentence in sent_tokenize(text) for word in word_tokenize(sentence)]\n",
    "    words=[word for word in words if len(word)>=3]\n",
    "    words=[word for word in words if word not in gensim.parsing.preprocessing.STOPWORDS]\n",
    "    words=[WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "    #Lemmatize\n",
    "#     words=[WordNetLemmatizer.lemmatize(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "reviews=df_work.review\n",
    "#tokenize text\n",
    "# reviews.apply(lambda x: x.lower())\n",
    "reviews=reviews.str.lower()\n",
    "reviews=reviews.str.replace('\\\"', '')\n",
    "reviews=reviews.str.replace(\"\\'\", '')\n",
    "reviews=reviews.str.replace('\\\"', '')\n",
    "reviews=reviews.str.replace('\\!', '')\n",
    "reviews=reviews.str.replace('\\/', ' ')\n",
    "reviews=reviews.str.replace(',', '')\n",
    "reviews=reviews.str.replace('(', '')\n",
    "reviews=reviews.str.replace(')', '')\n",
    "reviews=reviews.str.replace('.', '')\n",
    "reviews=reviews.str.replace('\\d+', '')\n",
    "reviews=reviews.replace(to_replace={'hik', 'walk','ing','run','trail', 'interesting',\\\n",
    "                                    'good','great','lot','recommend','area','park',\\\n",
    "                                    'love','like','way','easy'}, value='', regex=True)\n",
    "\n",
    "processed_text=[preprocess(text) for text in reviews]\n",
    "# lemmatized_text=[WordNetLemmatizer.lemmatize(text) for text in reviews]\n",
    "# WordNetLemmatizer.lemmatize('are')\n",
    "\n",
    "# reviews.head(50)\n",
    "print (processed_text[0], reviews[0])\n",
    "# print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Add processed text as a new column\n",
    "df_work['processed_text']= pd.Series(processed_text)\n",
    "df_work.head()\n",
    "#[' '.join(filter(None,filter(lambda word: word not in stop, line))) for line in data['text_data'].str.lower().str.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work.processed_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word countf for IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    " \n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    " \n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"./stop_words.txt\")\n",
    " \n",
    "#get the text column \n",
    "docs=df_work['processed_text'][0] #.tolist()\n",
    " \n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(cv.vocabulary_.keys())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(df_work['processed_text'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df_work['processed_text']]\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling based on this: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_text\n",
    "dictionary = gensim.corpora.Dictionary(processed_text)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=10000)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_text]\n",
    "print (len(bow_corpus))\n",
    "bow_corpus[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc_400 = bow_corpus[10]\n",
    "for i in range(len(bow_doc_400)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_400[i][0], \n",
    "                                               dictionary[bow_doc_400[i][0]], \n",
    "bow_doc_400[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train lda model using gensim.models.LdaMulticore and save it to ‘lda_model’\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "#Explore words in topic and relative weight\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA using TF-IDF\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance evaluation-LDA bag of words\n",
    "print (processed_text[40])\n",
    "for index, score in sorted(lda_model[bow_corpus[40]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Performance evaluation-LDA TF-IDF\n",
    "print (processed_text[40])\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[40]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run word2vec model and then save it\n",
    "# texts_stemmed = filter(None, [next_text.strip(' ').split(' ') for next_text in df_work['processed_text'][0]])\n",
    "# w2vmodel_stemmed = gensim.models.Word2Vec(texts_stemmed, size=100, window=5, min_count=5, workers=4)\n",
    "# w2vmodel_stemmed.save(savefolder+'w2v_stemmed_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
