{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#For text processing\n",
    "#Using this (https://bit.ly/2HvV2dx) as template\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer #, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chtra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chtra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dogs on leash',\n",
       " 'wheelchair friendly',\n",
       " 'kid friendly',\n",
       " 'stroller friendly',\n",
       " 'hiking',\n",
       " 'mountain biking',\n",
       " 'nature trips',\n",
       " 'snowshoeing',\n",
       " 'trail running',\n",
       " 'walking',\n",
       " 'forest',\n",
       " 'paved',\n",
       " 'views',\n",
       " 'snow']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df=pd.read_pickle('dftst.pkl')\n",
    "# df=pd.read_pickle('hiking_attributes_ontario1.pkl')\n",
    "# df=pd.read_pickle('alltrails_ontario.pkl')\n",
    "df=pd.read_pickle('alltrails_ontario.pkl')\n",
    "print (len(df))\n",
    "# df['review'][0]\n",
    "# df.head(50)\n",
    "\n",
    "df_work=df.copy()\n",
    "df_work.head(5) #trail_attributes[1] #.head(5)\n",
    "# df_work.columns\n",
    "df_work.review[0]\n",
    "df_work.trail_attributes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text=df_work.review\n",
    "review_list=pd.Series.tolist(review_text)\n",
    "def read_preproc(text):\n",
    "    for l in text:\n",
    "        yield gensim.utils.simple_preprocess(l)\n",
    "        \n",
    "preproc=list(read_preproc(review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for strollers bikes etc opposite side of river has a dirt path for mountain bikes dogs to avoid the heavier traffic on the paved paths which is nice tends to have sewer smells every single  i visit so itâ€™s not my first choice of  has a fully enclosed off leash dog  at the dvp end and a decent amount of free ing  only rode once but very well maintainedwent for a nice family  here a number of joggers and dog ers theres a little creek unsurprisingly which is fun for kids to look at and putter around nearnice long   for a  connects with the s in the don valleythe  is multi usage and s between don mills to victoria it is composed of a main course following a nice little river with two more hidden  hugging both sides of the valley cliffs and are under trees canopiesthose are more challenging and are share with mountain bike enthusiast but are less frequented and incredibly beautifuli  my beagle there as often as possible   for dogs\n"
     ]
    }
   ],
   "source": [
    "reviews=df_work.review\n",
    "reviews=reviews.str.lower()\n",
    "reviews=reviews.str.replace('\\\"', '')\n",
    "reviews=reviews.str.replace(\"\\'\", '')\n",
    "reviews=reviews.str.replace('\\\"', '')\n",
    "reviews=reviews.str.replace('\\!', '')\n",
    "reviews=reviews.str.replace('\\/', ' ')\n",
    "reviews=reviews.str.replace(',', '')\n",
    "reviews=reviews.str.replace('(', '')\n",
    "reviews=reviews.str.replace(')', '')\n",
    "reviews=reviews.str.replace('.', '')\n",
    "reviews=reviews.str.replace('\\d+', '')\n",
    "reviews=reviews.replace(to_replace={'hik', 'walk','run','trail', 'interesting',\\\n",
    "                                    'good','great','lot','recommend','area','park',\\\n",
    "                                    'love','like','way','easy','absolute','definite','time'}, value='', regex=True)\n",
    "#Pre-process\n",
    "def preprocess(text):\n",
    "    words=[word for sentence in sent_tokenize(text) for word in word_tokenize(sentence)]\n",
    "    words=[word for word in words if len(word)>3]\n",
    "    words=[word for word in words if word not in gensim.parsing.preprocessing.STOPWORDS]\n",
    "    #Lemmatize\n",
    "    words=[WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "review_list=pd.Series.tolist(reviews)\n",
    "print(review_list[0])\n",
    "\n",
    "preproc=list(preprocess(text) for text in review_list)\n",
    "# print (len(preproc), preproc[0])\n",
    "# print (preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:05:47,592 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-01-27 15:05:47,595 : INFO : collecting all words and their counts\n",
      "2019-01-27 15:05:47,596 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-27 15:05:47,614 : INFO : collected 7500 word types from a corpus of 44465 raw words and 437 sentences\n",
      "2019-01-27 15:05:47,616 : INFO : Loading a fresh vocabulary\n",
      "2019-01-27 15:05:47,637 : INFO : effective_min_count=2 retains 2904 unique words (38% of original 7500, drops 4596)\n",
      "2019-01-27 15:05:47,639 : INFO : effective_min_count=2 leaves 39869 word corpus (89% of original 44465, drops 4596)\n",
      "2019-01-27 15:05:47,663 : INFO : deleting the raw counts dictionary of 7500 items\n",
      "2019-01-27 15:05:47,664 : INFO : sample=0.001 downsamples 62 most-common words\n",
      "2019-01-27 15:05:47,668 : INFO : downsampling leaves estimated 35474 word corpus (89.0% of prior 39869)\n",
      "2019-01-27 15:05:47,695 : INFO : estimated required memory for 2904 words and 150 dimensions: 4936800 bytes\n",
      "2019-01-27 15:05:47,696 : INFO : resetting layer weights\n",
      "2019-01-27 15:05:47,801 : INFO : training model with 10 workers on 2904 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-27 15:05:47,812 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:47,822 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:47,831 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:47,841 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:47,842 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:47,847 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:47,857 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:47,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:47,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:47,878 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:47,882 : INFO : EPOCH - 1 : training on 44465 raw words (35407 effective words) took 0.1s, 474145 effective words/s\n",
      "2019-01-27 15:05:47,912 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:47,944 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:47,946 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:47,946 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:47,947 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:47,948 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:47,965 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:47,971 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:47,983 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:47,986 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:47,987 : INFO : EPOCH - 2 : training on 44465 raw words (35493 effective words) took 0.1s, 367688 effective words/s\n",
      "2019-01-27 15:05:48,000 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:48,028 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:48,036 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:48,043 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:48,049 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:48,051 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:48,067 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:48,070 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:48,074 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:48,077 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:48,078 : INFO : EPOCH - 3 : training on 44465 raw words (35563 effective words) took 0.1s, 423995 effective words/s\n",
      "2019-01-27 15:05:48,104 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:48,114 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:48,119 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:48,129 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:48,145 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:48,146 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:48,159 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:48,160 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:48,164 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:48,173 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:48,177 : INFO : EPOCH - 4 : training on 44465 raw words (35485 effective words) took 0.1s, 431377 effective words/s\n",
      "2019-01-27 15:05:48,193 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:48,202 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:48,207 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:48,224 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:48,240 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:48,244 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:48,257 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:48,266 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:48,269 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:48,274 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:48,275 : INFO : EPOCH - 5 : training on 44465 raw words (35486 effective words) took 0.1s, 397576 effective words/s\n",
      "2019-01-27 15:05:48,277 : INFO : training on a 222325 raw words (177434 effective words) took 0.5s, 373853 effective words/s\n",
      "2019-01-27 15:05:48,278 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-27 15:05:48,285 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-01-27 15:05:48,288 : INFO : training model with 10 workers on 2904 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-27 15:05:48,315 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:48,341 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:48,347 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:48,351 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:48,354 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:48,364 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:48,372 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:48,376 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:48,378 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:48,381 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:48,383 : INFO : EPOCH - 1 : training on 44465 raw words (35491 effective words) took 0.1s, 441359 effective words/s\n",
      "2019-01-27 15:05:48,396 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:48,411 : INFO : worker thread finished; awaiting finish of 8 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:05:48,414 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:48,427 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:48,438 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:48,451 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:48,458 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:48,467 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:48,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:48,478 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:48,479 : INFO : EPOCH - 2 : training on 44465 raw words (35505 effective words) took 0.1s, 394588 effective words/s\n",
      "2019-01-27 15:05:48,502 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:48,518 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:48,527 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:48,531 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:48,548 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:48,552 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:48,558 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:48,563 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:48,575 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:48,581 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:48,582 : INFO : EPOCH - 3 : training on 44465 raw words (35448 effective words) took 0.1s, 405699 effective words/s\n",
      "2019-01-27 15:05:48,604 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:48,629 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:48,635 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:48,662 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:48,666 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:48,674 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:48,695 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:48,711 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:48,721 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:48,727 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:48,729 : INFO : EPOCH - 4 : training on 44465 raw words (35455 effective words) took 0.1s, 262432 effective words/s\n",
      "2019-01-27 15:05:48,758 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:48,770 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:48,788 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:48,793 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:48,807 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:48,817 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:48,829 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:48,838 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:48,846 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:48,851 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:48,853 : INFO : EPOCH - 5 : training on 44465 raw words (35449 effective words) took 0.1s, 332979 effective words/s\n",
      "2019-01-27 15:05:48,903 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:48,922 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:48,944 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:48,953 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:48,956 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:48,962 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:48,995 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:48,997 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:49,000 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:49,023 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:49,024 : INFO : EPOCH - 6 : training on 44465 raw words (35448 effective words) took 0.1s, 241680 effective words/s\n",
      "2019-01-27 15:05:49,071 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:49,079 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:49,088 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:49,092 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:49,105 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:49,122 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:49,140 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:49,142 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:49,144 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:49,146 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:49,147 : INFO : EPOCH - 7 : training on 44465 raw words (35522 effective words) took 0.1s, 332973 effective words/s\n",
      "2019-01-27 15:05:49,187 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:49,194 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:49,208 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:49,226 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:49,231 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:49,271 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:49,286 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:49,287 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:49,304 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:49,309 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:49,310 : INFO : EPOCH - 8 : training on 44465 raw words (35476 effective words) took 0.1s, 273826 effective words/s\n",
      "2019-01-27 15:05:49,346 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-27 15:05:49,375 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:49,379 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:49,391 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:49,397 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:49,401 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:49,411 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:49,418 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:49,421 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:49,426 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:49,426 : INFO : EPOCH - 9 : training on 44465 raw words (35552 effective words) took 0.1s, 381005 effective words/s\n",
      "2019-01-27 15:05:49,448 : INFO : worker thread finished; awaiting finish of 9 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:05:49,457 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-27 15:05:49,473 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-27 15:05:49,475 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-27 15:05:49,488 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-27 15:05:49,503 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-27 15:05:49,508 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-27 15:05:49,524 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-27 15:05:49,530 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-27 15:05:49,537 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-27 15:05:49,538 : INFO : EPOCH - 10 : training on 44465 raw words (35437 effective words) took 0.1s, 364392 effective words/s\n",
      "2019-01-27 15:05:49,539 : INFO : training on a 444650 raw words (354783 effective words) took 1.2s, 283856 effective words/s\n",
      "2019-01-27 15:05:49,539 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(354783, 444650)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (preproc, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(preproc,total_examples=len(preproc),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dense', 0.9992693066596985),\n",
       " ('fallen', 0.9992460012435913),\n",
       " ('side', 0.9992207288742065),\n",
       " ('tall', 0.9992126822471619),\n",
       " ('marking', 0.9991759061813354),\n",
       " ('deciduous', 0.9991747736930847),\n",
       " ('grass', 0.9991742372512817),\n",
       " ('stream', 0.9991637468338013),\n",
       " ('overgrown', 0.9991631507873535),\n",
       " ('built', 0.9991576075553894)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"tree\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogsonleash', 'wheelchairfriendly', 'kidfriendly', 'strollerfriendly', 'hiking', 'mountainbiking', 'naturetrips', 'snowshoeing', 'trailrunning', 'walking', 'forest', 'paved', 'views', 'snow']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty</th>\n",
       "      <th>distance</th>\n",
       "      <th>elevation</th>\n",
       "      <th>name</th>\n",
       "      <th>nreviews</th>\n",
       "      <th>review</th>\n",
       "      <th>route_type</th>\n",
       "      <th>stars</th>\n",
       "      <th>trail_attributes</th>\n",
       "      <th>curated_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>5.6</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Taylor Creek Trail</td>\n",
       "      <td>23</td>\n",
       "      <td>Great for strollers, bikes etc. Opposite side ...</td>\n",
       "      <td>Out &amp; Back</td>\n",
       "      <td>3.7</td>\n",
       "      <td>[dogs on leash, wheelchair friendly, kid frien...</td>\n",
       "      <td>[dogsonleash, wheelchairfriendly, kidfriendly,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>4.7</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Hilton Falls Trail</td>\n",
       "      <td>238</td>\n",
       "      <td>What a gem! I was so pleasantly  surprised by ...</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.3</td>\n",
       "      <td>[dogs on leash, cross country skiing, fishing,...</td>\n",
       "      <td>[dogsonleash, crosscountryskiing, fishing, hik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>2.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Niagara Glen Trail</td>\n",
       "      <td>135</td>\n",
       "      <td>Beautiful area with several trails.  Loved exp...</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.7</td>\n",
       "      <td>[dogs on leash, kid friendly, hiking, nature t...</td>\n",
       "      <td>[dogsonleash, kidfriendly, hiking, naturetrips...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>7.2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>Nassagaweya and Bruce Trail Loop from Rattlesn...</td>\n",
       "      <td>170</td>\n",
       "      <td>Great views! We went in January so there weren...</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.2</td>\n",
       "      <td>[dogs on leash, kid friendly, hiking, nature t...</td>\n",
       "      <td>[dogsonleash, kidfriendly, hiking, naturetrips...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MODERATE</td>\n",
       "      <td>15.3</td>\n",
       "      <td>427.0</td>\n",
       "      <td>Lion's Head Loop Via Bruce Trail</td>\n",
       "      <td>117</td>\n",
       "      <td>Amazing trail with stunning lookouts. Hiked it...</td>\n",
       "      <td>Loop</td>\n",
       "      <td>4.8</td>\n",
       "      <td>[dogs on leash, hiking, nature trips, walking,...</td>\n",
       "      <td>[dogsonleash, hiking, naturetrips, walking, bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  difficulty  distance  elevation  \\\n",
       "0   MODERATE       5.6       32.0   \n",
       "1   MODERATE       4.7       64.0   \n",
       "2   MODERATE       2.3      100.0   \n",
       "3   MODERATE       7.2      128.0   \n",
       "4   MODERATE      15.3      427.0   \n",
       "\n",
       "                                                name nreviews  \\\n",
       "0                                 Taylor Creek Trail       23   \n",
       "1                                 Hilton Falls Trail      238   \n",
       "2                                 Niagara Glen Trail      135   \n",
       "3  Nassagaweya and Bruce Trail Loop from Rattlesn...      170   \n",
       "4                  Lion's Head Loop Via Bruce Trail       117   \n",
       "\n",
       "                                              review  route_type stars  \\\n",
       "0  Great for strollers, bikes etc. Opposite side ...  Out & Back   3.7   \n",
       "1  What a gem! I was so pleasantly  surprised by ...        Loop   4.3   \n",
       "2  Beautiful area with several trails.  Loved exp...        Loop   4.7   \n",
       "3  Great views! We went in January so there weren...        Loop   4.2   \n",
       "4  Amazing trail with stunning lookouts. Hiked it...        Loop   4.8   \n",
       "\n",
       "                                    trail_attributes  \\\n",
       "0  [dogs on leash, wheelchair friendly, kid frien...   \n",
       "1  [dogs on leash, cross country skiing, fishing,...   \n",
       "2  [dogs on leash, kid friendly, hiking, nature t...   \n",
       "3  [dogs on leash, kid friendly, hiking, nature t...   \n",
       "4  [dogs on leash, hiking, nature trips, walking,...   \n",
       "\n",
       "                                  curated_attributes  \n",
       "0  [dogsonleash, wheelchairfriendly, kidfriendly,...  \n",
       "1  [dogsonleash, crosscountryskiing, fishing, hik...  \n",
       "2  [dogsonleash, kidfriendly, hiking, naturetrips...  \n",
       "3  [dogsonleash, kidfriendly, hiking, naturetrips...  \n",
       "4  [dogsonleash, hiking, naturetrips, walking, bi...  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trying Jan27th - Trying vectorization on trail attributes\n",
    "trail_attributes=df_work.trail_attributes\n",
    "#Remove space to create list of single words ('dogsonleash' -> 'dogsonleash')\n",
    "hattrib=[]\n",
    "for l1 in trail_attributes:\n",
    "    hattrib.append([l2.replace(' ', '') for l2 in l1])\n",
    "print (hattrib[0])\n",
    "\n",
    "df_work['curated_attributes']=pd.Series(hattrib)\n",
    "df_work.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_stemming(text):\n",
    "#     return PorterStemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    words=[word for sentence in sent_tokenize(text) for word in word_tokenize(sentence)]\n",
    "    words=[word for word in words if len(word)>3]\n",
    "    words=[word for word in words if word not in gensim.parsing.preprocessing.STOPWORDS]\n",
    "    words=[WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "    #Lemmatize\n",
    "#     words=[WordNetLemmatizer.lemmatize(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part', 'paved', 'time', 'road', 'closed', 'couple', 'place', 'forcing', 'road', 'probably', 'route', 'bike', 'bikinga', 'paved', 'take', 'wooded', 'spent', 'hour', 'came', 'couple', 'deer', 'beaver', 'bunny', 'tiniest', 'squirrel', 'seen', 'chipmunk', 'bench', 'scenic', 'break', 'pit', 'bbqs', 'splash', 'jungle', 'near', 'main', 'lived', 'close', 'year', 'idea', 'place', 'pointed', 'outstarted', 'west', 'entrance', 'east', 'point', 'quick', 'small', 'forest', 'lakeshore', 'picnic', 'soft', 'mulcha', 'beautiful', 'traverse', 'beautiful', 'wooded', 'paved', 'path', 'wouldnt', 'know', 'youre', 'city', 'suddenly', 'lake', 'appears', 'view', 'beautiful', 'vista', 'ride', 'lakeshore', 'round', 'trip', 'starting', 'highland', 'creek', 'rode', 'east', 'itdid', 'quick', 'yesterday', 'amazing', 'wilderness', 'city', 'sung', 'skirted', 'tennis', 'court', 'built', 'campus', 'panam', 'game', 'amazing', 'growth', 'forest', 'wetland', 'highland', 'creek', 'choice', 'leaving', 'city', 'right', 'kingston', 'road', 'miss'] parts of this  are very  but it is almost all paved and there is too much time ing on roads the  was closed in a couple places forcing us back onto roads probably a  route to bike  as fun for bikinga paved  that takes you through  wooded s spent only an hour ing and came across a couple deer a beaver many bunnies and the tiniest squirrel i have ever seen no not a chipmunk several benches along the  to take scenic breaks there are fire pits and bbqs as well as a splash pad and jungle gym near the main ing  i have lived very close to this  for  years and had no idea this place was here until the app pointed it outstarted at west entrance of east point  for an  quick  in small forest and along the lakeshore can picnic soft mulcha beautiful   that traverses beautiful wooded  on  paved paths where you wouldnt even know youre in the city then suddenly the lake appears in front of view - a beautiful vista a ride along the lakeshore is ly as well we did about  km round trip starting from the highland creek ing  and rode east go for itdid about   of this  for a quick  yesterday  an amazing wilderness in the city  on the  back we sung by and skirted the tennis courts they have built on the u of t campus for the panam games  amazing thick of growth forest and wetlands on one side highland creek on the other - its a  choice for a  without leaving the city  ing is right along old kingston road - you cant miss it\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "reviews=df_work.review\n",
    "#tokenize text\n",
    "# reviews.apply(lambda x: x.lower())\n",
    "reviews=reviews.str.lower()\n",
    "reviews=reviews.str.replace('\\\"', '')\n",
    "reviews=reviews.str.replace(\"\\'\", '')\n",
    "reviews=reviews.str.replace('\\\"', '')\n",
    "reviews=reviews.str.replace('\\!', '')\n",
    "reviews=reviews.str.replace('\\/', ' ')\n",
    "reviews=reviews.str.replace(',', '')\n",
    "reviews=reviews.str.replace('(', '')\n",
    "reviews=reviews.str.replace(')', '')\n",
    "reviews=reviews.str.replace('.', '')\n",
    "reviews=reviews.str.replace('\\d+', '')\n",
    "reviews=reviews.replace(to_replace={'hik', 'walk','run','trail', 'interesting',\\\n",
    "                                    'good','great','lot','recommend','area','park',\\\n",
    "                                    'love','like','way','easy'}, value='', regex=True)\n",
    "\n",
    "processed_text=[preprocess(text) for text in reviews]\n",
    "# lemmatized_text=[WordNetLemmatizer.lemmatize(text) for text in reviews]\n",
    "# WordNetLemmatizer.lemmatize('are')\n",
    "\n",
    "# reviews.head(50)\n",
    "print (processed_text[0], reviews[0])\n",
    "# print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dogs on leash',\n",
       " 'kid friendly',\n",
       " 'birding',\n",
       " 'hiking',\n",
       " 'mountain biking',\n",
       " 'nature trips',\n",
       " 'road biking',\n",
       " 'snowshoeing',\n",
       " 'trail running',\n",
       " 'walking',\n",
       " 'forest',\n",
       " 'views',\n",
       " 'waterfall',\n",
       " 'wild flowers',\n",
       " 'wildlife']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add processed text as a new column\n",
    "df_work['processed_text']= pd.Series(processed_text)\n",
    "df_work['trail_attributes'][10]\n",
    "#[' '.join(filter(None,filter(lambda word: word not in stop, line))) for line in data['text_data'].str.lower().str.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word countf for IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    " \n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    " \n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"./stop_words.txt\")\n",
    " \n",
    "#get the text column \n",
    "docs=df_work['processed_text'][0] #.tolist()\n",
    " \n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['part',\n",
       " 'paved',\n",
       " 'time',\n",
       " 'road',\n",
       " 'closed',\n",
       " 'couple',\n",
       " 'place',\n",
       " 'forcing',\n",
       " 'route',\n",
       " 'bike',\n",
       " 'bikinga',\n",
       " 'take',\n",
       " 'wooded',\n",
       " 'spent',\n",
       " 'hour',\n",
       " 'came',\n",
       " 'deer',\n",
       " 'beaver',\n",
       " 'bunny',\n",
       " 'tiniest']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:05:51,086 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-01-27 15:05:51,253 : INFO : built Dictionary(7534 unique tokens: ['amazing', 'appears', 'bbqs', 'beautiful', 'beaver']...) from 437 documents (total 45155 corpus positions)\n",
      "2019-01-27 15:05:51,335 : INFO : saving Dictionary object under dictionary.gensim, separately None\n",
      "2019-01-27 15:05:51,346 : INFO : saved dictionary.gensim\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(df_work['processed_text'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df_work['processed_text']]\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:05:51,358 : INFO : using symmetric alpha at 0.2\n",
      "2019-01-27 15:05:51,360 : INFO : using symmetric eta at 0.2\n",
      "2019-01-27 15:05:51,365 : INFO : using serial LDA version on this node\n",
      "2019-01-27 15:05:51,372 : INFO : running online (multi-pass) LDA training, 5 topics, 15 passes over the supplied corpus of 437 documents, updating model once every 437 documents, evaluating perplexity every 437 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-01-27 15:05:52,056 : INFO : -10.031 per-word bound, 1046.5 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:05:52,057 : INFO : PROGRESS: pass 0, at document #437/437\n",
      "2019-01-27 15:05:52,627 : INFO : topic #0 (0.200): 0.017*\"nice\" + 0.013*\"beautiful\" + 0.009*\"time\" + 0.008*\"little\" + 0.007*\"fall\" + 0.007*\"view\" + 0.007*\"place\" + 0.006*\"river\" + 0.006*\"path\" + 0.006*\"road\"\n",
      "2019-01-27 15:05:52,628 : INFO : topic #1 (0.200): 0.015*\"view\" + 0.013*\"nice\" + 0.012*\"beautiful\" + 0.008*\"loop\" + 0.008*\"little\" + 0.007*\"time\" + 0.007*\"fall\" + 0.007*\"rock\" + 0.006*\"place\" + 0.005*\"lake\"\n",
      "2019-01-27 15:05:52,631 : INFO : topic #2 (0.200): 0.012*\"nice\" + 0.011*\"view\" + 0.010*\"fall\" + 0.006*\"beautiful\" + 0.006*\"amazing\" + 0.005*\"time\" + 0.005*\"road\" + 0.005*\"marked\" + 0.005*\"hour\" + 0.005*\"spot\"\n",
      "2019-01-27 15:05:52,632 : INFO : topic #3 (0.200): 0.021*\"view\" + 0.019*\"nice\" + 0.012*\"beautiful\" + 0.010*\"lake\" + 0.008*\"time\" + 0.006*\"fall\" + 0.006*\"loop\" + 0.006*\"marked\" + 0.005*\"definitely\" + 0.005*\"rock\"\n",
      "2019-01-27 15:05:52,634 : INFO : topic #4 (0.200): 0.008*\"beautiful\" + 0.007*\"nice\" + 0.007*\"view\" + 0.007*\"time\" + 0.006*\"little\" + 0.005*\"river\" + 0.005*\"place\" + 0.005*\"loop\" + 0.005*\"people\" + 0.005*\"dog\"\n",
      "2019-01-27 15:05:52,636 : INFO : topic diff=2.221260, rho=1.000000\n",
      "2019-01-27 15:05:53,383 : INFO : -7.961 per-word bound, 249.2 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:05:53,386 : INFO : PROGRESS: pass 1, at document #437/437\n",
      "2019-01-27 15:05:53,763 : INFO : topic #0 (0.200): 0.018*\"nice\" + 0.012*\"beautiful\" + 0.009*\"time\" + 0.008*\"little\" + 0.008*\"river\" + 0.007*\"fall\" + 0.007*\"place\" + 0.006*\"path\" + 0.006*\"road\" + 0.006*\"view\"\n",
      "2019-01-27 15:05:53,764 : INFO : topic #1 (0.200): 0.015*\"view\" + 0.013*\"nice\" + 0.012*\"beautiful\" + 0.008*\"loop\" + 0.008*\"rock\" + 0.007*\"fall\" + 0.007*\"little\" + 0.007*\"time\" + 0.006*\"place\" + 0.006*\"lake\"\n",
      "2019-01-27 15:05:53,765 : INFO : topic #2 (0.200): 0.011*\"nice\" + 0.010*\"fall\" + 0.009*\"view\" + 0.005*\"amazing\" + 0.005*\"beautiful\" + 0.005*\"marked\" + 0.005*\"road\" + 0.004*\"spot\" + 0.004*\"time\" + 0.004*\"hour\"\n",
      "2019-01-27 15:05:53,767 : INFO : topic #3 (0.200): 0.022*\"view\" + 0.018*\"nice\" + 0.013*\"beautiful\" + 0.010*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.006*\"fall\" + 0.006*\"rock\" + 0.006*\"definitely\" + 0.005*\"marked\"\n",
      "2019-01-27 15:05:53,769 : INFO : topic #4 (0.200): 0.007*\"beautiful\" + 0.006*\"time\" + 0.006*\"section\" + 0.006*\"nice\" + 0.005*\"river\" + 0.005*\"cave\" + 0.005*\"little\" + 0.005*\"place\" + 0.005*\"road\" + 0.005*\"view\"\n",
      "2019-01-27 15:05:53,770 : INFO : topic diff=0.590560, rho=0.577350\n",
      "2019-01-27 15:05:54,305 : INFO : -7.822 per-word bound, 226.3 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:05:54,305 : INFO : PROGRESS: pass 2, at document #437/437\n",
      "2019-01-27 15:05:54,582 : INFO : topic #0 (0.200): 0.018*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.008*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.006*\"path\" + 0.006*\"road\" + 0.006*\"dog\"\n",
      "2019-01-27 15:05:54,584 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"nice\" + 0.012*\"beautiful\" + 0.008*\"rock\" + 0.008*\"loop\" + 0.007*\"fall\" + 0.007*\"time\" + 0.007*\"little\" + 0.006*\"lake\" + 0.006*\"place\"\n",
      "2019-01-27 15:05:54,585 : INFO : topic #2 (0.200): 0.011*\"nice\" + 0.010*\"fall\" + 0.007*\"view\" + 0.005*\"marked\" + 0.005*\"road\" + 0.005*\"amazing\" + 0.004*\"beautiful\" + 0.004*\"spot\" + 0.004*\"path\" + 0.004*\"time\"\n",
      "2019-01-27 15:05:54,586 : INFO : topic #3 (0.200): 0.023*\"view\" + 0.017*\"nice\" + 0.013*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.006*\"rock\" + 0.006*\"fall\" + 0.006*\"definitely\" + 0.006*\"hour\"\n",
      "2019-01-27 15:05:54,588 : INFO : topic #4 (0.200): 0.006*\"section\" + 0.006*\"time\" + 0.006*\"beautiful\" + 0.006*\"cave\" + 0.006*\"road\" + 0.005*\"river\" + 0.005*\"place\" + 0.005*\"nice\" + 0.005*\"little\" + 0.005*\"dog\"\n",
      "2019-01-27 15:05:54,589 : INFO : topic diff=0.371335, rho=0.500000\n",
      "2019-01-27 15:05:55,248 : INFO : -7.771 per-word bound, 218.4 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:05:55,249 : INFO : PROGRESS: pass 3, at document #437/437\n",
      "2019-01-27 15:05:55,537 : INFO : topic #0 (0.200): 0.019*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.008*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.006*\"road\" + 0.006*\"path\" + 0.006*\"dog\"\n",
      "2019-01-27 15:05:55,538 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"nice\" + 0.012*\"beautiful\" + 0.008*\"rock\" + 0.007*\"fall\" + 0.007*\"loop\" + 0.007*\"time\" + 0.007*\"little\" + 0.007*\"lake\" + 0.006*\"lookout\"\n",
      "2019-01-27 15:05:55,539 : INFO : topic #2 (0.200): 0.010*\"nice\" + 0.010*\"fall\" + 0.006*\"view\" + 0.005*\"marked\" + 0.005*\"road\" + 0.004*\"amazing\" + 0.004*\"spot\" + 0.004*\"beautiful\" + 0.004*\"little\" + 0.004*\"path\"\n",
      "2019-01-27 15:05:55,540 : INFO : topic #3 (0.200): 0.023*\"view\" + 0.017*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.006*\"rock\" + 0.006*\"fall\" + 0.006*\"hour\" + 0.006*\"definitely\"\n",
      "2019-01-27 15:05:55,541 : INFO : topic #4 (0.200): 0.007*\"section\" + 0.006*\"time\" + 0.006*\"road\" + 0.006*\"cave\" + 0.006*\"river\" + 0.005*\"beautiful\" + 0.005*\"place\" + 0.004*\"little\" + 0.004*\"dog\" + 0.004*\"nice\"\n",
      "2019-01-27 15:05:55,542 : INFO : topic diff=0.229199, rho=0.447214\n",
      "2019-01-27 15:05:56,086 : INFO : -7.751 per-word bound, 215.3 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:05:56,087 : INFO : PROGRESS: pass 4, at document #437/437\n",
      "2019-01-27 15:05:56,391 : INFO : topic #0 (0.200): 0.019*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.008*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.006*\"road\" + 0.006*\"path\" + 0.006*\"dog\"\n",
      "2019-01-27 15:05:56,392 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.012*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.007*\"fall\" + 0.007*\"loop\" + 0.007*\"time\" + 0.007*\"lake\" + 0.007*\"little\" + 0.006*\"lookout\"\n",
      "2019-01-27 15:05:56,393 : INFO : topic #2 (0.200): 0.010*\"fall\" + 0.010*\"nice\" + 0.006*\"view\" + 0.005*\"marked\" + 0.005*\"road\" + 0.004*\"amazing\" + 0.004*\"little\" + 0.004*\"spot\" + 0.003*\"path\" + 0.003*\"beautiful\"\n",
      "2019-01-27 15:05:56,394 : INFO : topic #3 (0.200): 0.023*\"view\" + 0.017*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.006*\"rock\" + 0.006*\"hour\" + 0.006*\"fall\" + 0.006*\"definitely\"\n",
      "2019-01-27 15:05:56,396 : INFO : topic #4 (0.200): 0.007*\"section\" + 0.006*\"time\" + 0.006*\"road\" + 0.006*\"river\" + 0.006*\"cave\" + 0.005*\"place\" + 0.004*\"beautiful\" + 0.004*\"dog\" + 0.004*\"little\" + 0.004*\"nice\"\n",
      "2019-01-27 15:05:56,397 : INFO : topic diff=0.145062, rho=0.408248\n",
      "2019-01-27 15:05:56,906 : INFO : -7.741 per-word bound, 213.9 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:05:56,907 : INFO : PROGRESS: pass 5, at document #437/437\n",
      "2019-01-27 15:05:57,323 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.008*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.007*\"road\" + 0.006*\"path\" + 0.006*\"dog\"\n",
      "2019-01-27 15:05:57,326 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.012*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.008*\"fall\" + 0.007*\"loop\" + 0.007*\"time\" + 0.007*\"lake\" + 0.007*\"little\" + 0.006*\"lookout\"\n",
      "2019-01-27 15:05:57,328 : INFO : topic #2 (0.200): 0.010*\"fall\" + 0.009*\"nice\" + 0.005*\"view\" + 0.005*\"marked\" + 0.005*\"road\" + 0.004*\"little\" + 0.004*\"amazing\" + 0.003*\"spot\" + 0.003*\"path\" + 0.003*\"forest\"\n",
      "2019-01-27 15:05:57,331 : INFO : topic #3 (0.200): 0.023*\"view\" + 0.017*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.006*\"rock\" + 0.006*\"hour\" + 0.006*\"fall\" + 0.006*\"definitely\"\n",
      "2019-01-27 15:05:57,339 : INFO : topic #4 (0.200): 0.007*\"section\" + 0.006*\"road\" + 0.006*\"time\" + 0.006*\"river\" + 0.006*\"cave\" + 0.005*\"place\" + 0.004*\"dog\" + 0.004*\"little\" + 0.004*\"beautiful\" + 0.004*\"bridge\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:05:57,342 : INFO : topic diff=0.094789, rho=0.377964\n",
      "2019-01-27 15:05:58,162 : INFO : -7.735 per-word bound, 213.0 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:05:58,162 : INFO : PROGRESS: pass 6, at document #437/437\n",
      "2019-01-27 15:05:58,481 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.008*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.007*\"road\" + 0.006*\"path\" + 0.006*\"dog\"\n",
      "2019-01-27 15:05:58,482 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.008*\"fall\" + 0.007*\"time\" + 0.007*\"loop\" + 0.007*\"lake\" + 0.006*\"lookout\" + 0.006*\"little\"\n",
      "2019-01-27 15:05:58,484 : INFO : topic #2 (0.200): 0.010*\"fall\" + 0.008*\"nice\" + 0.005*\"marked\" + 0.005*\"view\" + 0.005*\"road\" + 0.004*\"little\" + 0.004*\"amazing\" + 0.003*\"path\" + 0.003*\"tews\" + 0.003*\"spot\"\n",
      "2019-01-27 15:05:58,486 : INFO : topic #3 (0.200): 0.023*\"view\" + 0.017*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.006*\"hour\" + 0.006*\"definitely\" + 0.006*\"fall\"\n",
      "2019-01-27 15:05:58,487 : INFO : topic #4 (0.200): 0.008*\"section\" + 0.006*\"road\" + 0.006*\"river\" + 0.006*\"time\" + 0.006*\"cave\" + 0.005*\"place\" + 0.004*\"dog\" + 0.004*\"little\" + 0.004*\"bridge\" + 0.004*\"beautiful\"\n",
      "2019-01-27 15:05:58,489 : INFO : topic diff=0.064155, rho=0.353553\n",
      "2019-01-27 15:05:58,969 : INFO : -7.731 per-word bound, 212.5 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:05:58,970 : INFO : PROGRESS: pass 7, at document #437/437\n",
      "2019-01-27 15:05:59,605 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.008*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.007*\"road\" + 0.006*\"dog\" + 0.006*\"path\"\n",
      "2019-01-27 15:05:59,607 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.008*\"fall\" + 0.007*\"time\" + 0.007*\"loop\" + 0.007*\"lake\" + 0.007*\"lookout\" + 0.006*\"little\"\n",
      "2019-01-27 15:05:59,608 : INFO : topic #2 (0.200): 0.010*\"fall\" + 0.008*\"nice\" + 0.005*\"marked\" + 0.004*\"road\" + 0.004*\"view\" + 0.004*\"little\" + 0.003*\"amazing\" + 0.003*\"tews\" + 0.003*\"path\" + 0.003*\"forest\"\n",
      "2019-01-27 15:05:59,610 : INFO : topic #3 (0.200): 0.023*\"view\" + 0.016*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.006*\"hour\" + 0.006*\"amazing\" + 0.006*\"definitely\"\n",
      "2019-01-27 15:05:59,614 : INFO : topic #4 (0.200): 0.008*\"section\" + 0.006*\"road\" + 0.006*\"river\" + 0.006*\"time\" + 0.006*\"cave\" + 0.004*\"place\" + 0.004*\"dog\" + 0.004*\"little\" + 0.004*\"bridge\" + 0.003*\"beautiful\"\n",
      "2019-01-27 15:05:59,615 : INFO : topic diff=0.045094, rho=0.333333\n",
      "2019-01-27 15:06:00,132 : INFO : -7.729 per-word bound, 212.1 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:06:00,133 : INFO : PROGRESS: pass 8, at document #437/437\n",
      "2019-01-27 15:06:00,400 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.008*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.007*\"road\" + 0.007*\"dog\" + 0.006*\"path\"\n",
      "2019-01-27 15:06:00,401 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.008*\"fall\" + 0.007*\"time\" + 0.007*\"loop\" + 0.007*\"lake\" + 0.007*\"lookout\" + 0.006*\"little\"\n",
      "2019-01-27 15:06:00,402 : INFO : topic #2 (0.200): 0.010*\"fall\" + 0.007*\"nice\" + 0.005*\"marked\" + 0.004*\"road\" + 0.004*\"view\" + 0.004*\"little\" + 0.003*\"amazing\" + 0.003*\"tews\" + 0.003*\"path\" + 0.003*\"forest\"\n",
      "2019-01-27 15:06:00,404 : INFO : topic #3 (0.200): 0.023*\"view\" + 0.016*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.006*\"hour\" + 0.006*\"amazing\" + 0.006*\"definitely\"\n",
      "2019-01-27 15:06:00,405 : INFO : topic #4 (0.200): 0.008*\"section\" + 0.006*\"road\" + 0.006*\"river\" + 0.006*\"time\" + 0.006*\"cave\" + 0.004*\"place\" + 0.004*\"bridge\" + 0.004*\"little\" + 0.004*\"dog\" + 0.003*\"maintained\"\n",
      "2019-01-27 15:06:00,407 : INFO : topic diff=0.033163, rho=0.316228\n",
      "2019-01-27 15:06:01,067 : INFO : -7.727 per-word bound, 211.8 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:06:01,068 : INFO : PROGRESS: pass 9, at document #437/437\n",
      "2019-01-27 15:06:01,358 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.008*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.007*\"road\" + 0.007*\"dog\" + 0.006*\"path\"\n",
      "2019-01-27 15:06:01,359 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.008*\"fall\" + 0.007*\"time\" + 0.007*\"loop\" + 0.007*\"lake\" + 0.007*\"lookout\" + 0.006*\"little\"\n",
      "2019-01-27 15:06:01,360 : INFO : topic #2 (0.200): 0.010*\"fall\" + 0.007*\"nice\" + 0.005*\"marked\" + 0.004*\"road\" + 0.004*\"little\" + 0.004*\"view\" + 0.003*\"tews\" + 0.003*\"amazing\" + 0.003*\"path\" + 0.003*\"forest\"\n",
      "2019-01-27 15:06:01,361 : INFO : topic #3 (0.200): 0.024*\"view\" + 0.016*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.006*\"hour\" + 0.006*\"amazing\" + 0.006*\"fall\"\n",
      "2019-01-27 15:06:01,362 : INFO : topic #4 (0.200): 0.008*\"section\" + 0.006*\"river\" + 0.006*\"road\" + 0.006*\"time\" + 0.006*\"cave\" + 0.004*\"place\" + 0.004*\"bridge\" + 0.004*\"little\" + 0.004*\"dog\" + 0.003*\"maintained\"\n",
      "2019-01-27 15:06:01,363 : INFO : topic diff=0.025488, rho=0.301511\n",
      "2019-01-27 15:06:01,845 : INFO : -7.725 per-word bound, 211.6 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:06:01,845 : INFO : PROGRESS: pass 10, at document #437/437\n",
      "2019-01-27 15:06:02,248 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.008*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.007*\"road\" + 0.007*\"dog\" + 0.006*\"path\"\n",
      "2019-01-27 15:06:02,249 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.008*\"fall\" + 0.007*\"loop\" + 0.007*\"time\" + 0.007*\"lake\" + 0.007*\"lookout\" + 0.006*\"little\"\n",
      "2019-01-27 15:06:02,256 : INFO : topic #2 (0.200): 0.010*\"fall\" + 0.007*\"nice\" + 0.005*\"marked\" + 0.004*\"road\" + 0.004*\"little\" + 0.004*\"tews\" + 0.003*\"view\" + 0.003*\"amazing\" + 0.003*\"path\" + 0.003*\"forest\"\n",
      "2019-01-27 15:06:02,260 : INFO : topic #3 (0.200): 0.024*\"view\" + 0.016*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.006*\"hour\" + 0.006*\"amazing\" + 0.006*\"fall\"\n",
      "2019-01-27 15:06:02,264 : INFO : topic #4 (0.200): 0.008*\"section\" + 0.006*\"river\" + 0.006*\"road\" + 0.006*\"time\" + 0.005*\"cave\" + 0.004*\"place\" + 0.004*\"bridge\" + 0.004*\"little\" + 0.003*\"dog\" + 0.003*\"maintained\"\n",
      "2019-01-27 15:06:02,265 : INFO : topic diff=0.020490, rho=0.288675\n",
      "2019-01-27 15:06:02,854 : INFO : -7.724 per-word bound, 211.4 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:06:02,855 : INFO : PROGRESS: pass 11, at document #437/437\n",
      "2019-01-27 15:06:03,266 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.009*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.007*\"road\" + 0.007*\"dog\" + 0.006*\"path\"\n",
      "2019-01-27 15:06:03,268 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.008*\"fall\" + 0.007*\"loop\" + 0.007*\"time\" + 0.007*\"lake\" + 0.007*\"lookout\" + 0.006*\"little\"\n",
      "2019-01-27 15:06:03,270 : INFO : topic #2 (0.200): 0.010*\"fall\" + 0.006*\"nice\" + 0.005*\"marked\" + 0.004*\"road\" + 0.004*\"little\" + 0.004*\"tews\" + 0.003*\"amazing\" + 0.003*\"view\" + 0.003*\"people\" + 0.003*\"path\"\n",
      "2019-01-27 15:06:03,270 : INFO : topic #3 (0.200): 0.024*\"view\" + 0.016*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.006*\"hour\" + 0.006*\"amazing\" + 0.006*\"fall\"\n",
      "2019-01-27 15:06:03,272 : INFO : topic #4 (0.200): 0.008*\"section\" + 0.006*\"river\" + 0.006*\"road\" + 0.006*\"time\" + 0.005*\"cave\" + 0.004*\"bridge\" + 0.004*\"place\" + 0.004*\"little\" + 0.003*\"maintained\" + 0.003*\"long\"\n",
      "2019-01-27 15:06:03,273 : INFO : topic diff=0.017147, rho=0.277350\n",
      "2019-01-27 15:06:03,894 : INFO : -7.723 per-word bound, 211.3 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:06:03,895 : INFO : PROGRESS: pass 12, at document #437/437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:06:04,223 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.009*\"river\" + 0.008*\"fall\" + 0.008*\"little\" + 0.007*\"place\" + 0.007*\"road\" + 0.007*\"dog\" + 0.007*\"path\"\n",
      "2019-01-27 15:06:04,224 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.008*\"fall\" + 0.007*\"loop\" + 0.007*\"time\" + 0.007*\"lake\" + 0.007*\"lookout\" + 0.006*\"little\"\n",
      "2019-01-27 15:06:04,226 : INFO : topic #2 (0.200): 0.010*\"fall\" + 0.006*\"nice\" + 0.005*\"marked\" + 0.004*\"road\" + 0.004*\"little\" + 0.004*\"tews\" + 0.003*\"amazing\" + 0.003*\"view\" + 0.003*\"people\" + 0.003*\"dundas\"\n",
      "2019-01-27 15:06:04,227 : INFO : topic #3 (0.200): 0.024*\"view\" + 0.016*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.006*\"hour\" + 0.006*\"amazing\" + 0.006*\"fall\"\n",
      "2019-01-27 15:06:04,228 : INFO : topic #4 (0.200): 0.008*\"section\" + 0.006*\"river\" + 0.006*\"road\" + 0.006*\"time\" + 0.005*\"cave\" + 0.004*\"bridge\" + 0.004*\"place\" + 0.004*\"little\" + 0.003*\"maintained\" + 0.003*\"long\"\n",
      "2019-01-27 15:06:04,230 : INFO : topic diff=0.014802, rho=0.267261\n",
      "2019-01-27 15:06:05,026 : INFO : -7.722 per-word bound, 211.1 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:06:05,027 : INFO : PROGRESS: pass 13, at document #437/437\n",
      "2019-01-27 15:06:05,314 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.009*\"river\" + 0.008*\"fall\" + 0.007*\"little\" + 0.007*\"place\" + 0.007*\"dog\" + 0.007*\"road\" + 0.007*\"path\"\n",
      "2019-01-27 15:06:05,315 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.008*\"fall\" + 0.007*\"loop\" + 0.007*\"time\" + 0.007*\"lake\" + 0.007*\"lookout\" + 0.006*\"little\"\n",
      "2019-01-27 15:06:05,316 : INFO : topic #2 (0.200): 0.009*\"fall\" + 0.006*\"nice\" + 0.005*\"marked\" + 0.004*\"road\" + 0.004*\"little\" + 0.004*\"tews\" + 0.003*\"amazing\" + 0.003*\"people\" + 0.003*\"dundas\" + 0.003*\"sign\"\n",
      "2019-01-27 15:06:05,317 : INFO : topic #3 (0.200): 0.024*\"view\" + 0.016*\"nice\" + 0.014*\"beautiful\" + 0.011*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.006*\"hour\" + 0.006*\"fall\" + 0.006*\"amazing\"\n",
      "2019-01-27 15:06:05,318 : INFO : topic #4 (0.200): 0.008*\"section\" + 0.006*\"river\" + 0.006*\"road\" + 0.006*\"time\" + 0.005*\"cave\" + 0.004*\"bridge\" + 0.004*\"place\" + 0.004*\"little\" + 0.003*\"maintained\" + 0.003*\"long\"\n",
      "2019-01-27 15:06:05,319 : INFO : topic diff=0.013121, rho=0.258199\n",
      "2019-01-27 15:06:05,775 : INFO : -7.721 per-word bound, 211.0 perplexity estimate based on a held-out corpus of 437 documents with 45155 words\n",
      "2019-01-27 15:06:05,776 : INFO : PROGRESS: pass 14, at document #437/437\n",
      "2019-01-27 15:06:06,035 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.009*\"river\" + 0.008*\"fall\" + 0.007*\"little\" + 0.007*\"place\" + 0.007*\"dog\" + 0.007*\"road\" + 0.007*\"path\"\n",
      "2019-01-27 15:06:06,037 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\" + 0.008*\"fall\" + 0.007*\"loop\" + 0.007*\"time\" + 0.007*\"lake\" + 0.007*\"lookout\" + 0.006*\"little\"\n",
      "2019-01-27 15:06:06,038 : INFO : topic #2 (0.200): 0.009*\"fall\" + 0.006*\"nice\" + 0.005*\"marked\" + 0.004*\"road\" + 0.004*\"little\" + 0.004*\"tews\" + 0.003*\"amazing\" + 0.003*\"dundas\" + 0.003*\"people\" + 0.003*\"sign\"\n",
      "2019-01-27 15:06:06,040 : INFO : topic #3 (0.200): 0.024*\"view\" + 0.016*\"nice\" + 0.014*\"beautiful\" + 0.012*\"lake\" + 0.009*\"time\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.006*\"hour\" + 0.006*\"fall\" + 0.006*\"amazing\"\n",
      "2019-01-27 15:06:06,041 : INFO : topic #4 (0.200): 0.008*\"section\" + 0.006*\"river\" + 0.006*\"road\" + 0.005*\"time\" + 0.005*\"cave\" + 0.004*\"bridge\" + 0.004*\"place\" + 0.004*\"little\" + 0.003*\"forest\" + 0.003*\"maintained\"\n",
      "2019-01-27 15:06:06,042 : INFO : topic diff=0.011886, rho=0.250000\n",
      "2019-01-27 15:06:06,046 : INFO : saving LdaState object under model5.gensim.state, separately None\n",
      "2019-01-27 15:06:06,058 : INFO : saved model5.gensim.state\n",
      "2019-01-27 15:06:06,071 : INFO : saving LdaModel object under model5.gensim, separately ['expElogbeta', 'sstats']\n",
      "2019-01-27 15:06:06,072 : INFO : storing np array 'expElogbeta' to model5.gensim.expElogbeta.npy\n",
      "2019-01-27 15:06:06,083 : INFO : not storing attribute id2word\n",
      "2019-01-27 15:06:06,084 : INFO : not storing attribute dispatcher\n",
      "2019-01-27 15:06:06,085 : INFO : not storing attribute state\n",
      "2019-01-27 15:06:06,099 : INFO : saved model5.gensim\n",
      "2019-01-27 15:06:06,102 : INFO : topic #0 (0.200): 0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.009*\"river\"\n",
      "2019-01-27 15:06:06,103 : INFO : topic #1 (0.200): 0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\"\n",
      "2019-01-27 15:06:06,105 : INFO : topic #2 (0.200): 0.009*\"fall\" + 0.006*\"nice\" + 0.005*\"marked\" + 0.004*\"road\"\n",
      "2019-01-27 15:06:06,106 : INFO : topic #3 (0.200): 0.024*\"view\" + 0.016*\"nice\" + 0.014*\"beautiful\" + 0.012*\"lake\"\n",
      "2019-01-27 15:06:06,108 : INFO : topic #4 (0.200): 0.008*\"section\" + 0.006*\"river\" + 0.006*\"road\" + 0.005*\"time\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.020*\"nice\" + 0.011*\"beautiful\" + 0.009*\"time\" + 0.009*\"river\"')\n",
      "(1, '0.016*\"view\" + 0.013*\"beautiful\" + 0.012*\"nice\" + 0.008*\"rock\"')\n",
      "(2, '0.009*\"fall\" + 0.006*\"nice\" + 0.005*\"marked\" + 0.004*\"road\"')\n",
      "(3, '0.024*\"view\" + 0.016*\"nice\" + 0.014*\"beautiful\" + 0.012*\"lake\"')\n",
      "(4, '0.008*\"section\" + 0.006*\"river\" + 0.006*\"road\" + 0.005*\"time\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling based on this: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:06:06,118 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-01-27 15:06:06,218 : INFO : built Dictionary(7534 unique tokens: ['amazing', 'appears', 'bbqs', 'beautiful', 'beaver']...) from 437 documents (total 45155 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 amazing\n",
      "1 appears\n",
      "2 bbqs\n",
      "3 beautiful\n",
      "4 beaver\n",
      "5 bench\n",
      "6 bike\n",
      "7 bikinga\n",
      "8 break\n",
      "9 built\n",
      "10 bunny\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7534"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processed_text\n",
    "dictionary = gensim.corpora.Dictionary(processed_text)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:06:06,254 : INFO : discarding 7071 tokens: [('appears', 7), ('bbqs', 2), ('beautiful', 251), ('bikinga', 2), ('built', 11), ('bunny', 3), ('campus', 2), ('choice', 12), ('court', 3), ('forcing', 2)]...\n",
      "2019-01-27 15:06:06,256 : INFO : keeping 463 tokens which were in no less than 15 and no more than 218 (=50.0%) documents\n",
      "2019-01-27 15:06:06,261 : INFO : resulting dictionary: Dictionary(463 unique tokens: ['amazing', 'beaver', 'bench', 'bike', 'break']...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=10000)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 2),\n",
       " (7, 1),\n",
       " (9, 2),\n",
       " (10, 1),\n",
       " (12, 1),\n",
       " (15, 1),\n",
       " (16, 2),\n",
       " (19, 1),\n",
       " (20, 3),\n",
       " (26, 1),\n",
       " (27, 6),\n",
       " (28, 1),\n",
       " (31, 2),\n",
       " (36, 1),\n",
       " (38, 2),\n",
       " (42, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (52, 1),\n",
       " (56, 1),\n",
       " (57, 3),\n",
       " (63, 2),\n",
       " (64, 1),\n",
       " (66, 1),\n",
       " (69, 2),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (76, 2),\n",
       " (79, 1),\n",
       " (81, 1),\n",
       " (83, 1),\n",
       " (84, 2),\n",
       " (88, 1),\n",
       " (89, 2),\n",
       " (92, 1),\n",
       " (123, 3),\n",
       " (127, 2),\n",
       " (133, 1),\n",
       " (135, 1),\n",
       " (141, 5),\n",
       " (142, 1),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (149, 1),\n",
       " (150, 2),\n",
       " (154, 1),\n",
       " (155, 3),\n",
       " (156, 1),\n",
       " (159, 1),\n",
       " (185, 1),\n",
       " (189, 2),\n",
       " (193, 2),\n",
       " (195, 5),\n",
       " (198, 2),\n",
       " (199, 1),\n",
       " (202, 1),\n",
       " (224, 1),\n",
       " (228, 2),\n",
       " (231, 1),\n",
       " (235, 3),\n",
       " (238, 2),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (259, 2),\n",
       " (260, 1),\n",
       " (264, 1),\n",
       " (268, 1),\n",
       " (279, 2),\n",
       " (285, 1),\n",
       " (292, 2),\n",
       " (306, 1),\n",
       " (314, 1),\n",
       " (318, 1),\n",
       " (320, 1),\n",
       " (321, 1),\n",
       " (327, 3),\n",
       " (330, 1),\n",
       " (332, 1),\n",
       " (333, 2),\n",
       " (334, 2),\n",
       " (341, 1),\n",
       " (343, 1),\n",
       " (344, 1),\n",
       " (352, 2),\n",
       " (355, 1),\n",
       " (357, 2),\n",
       " (360, 1),\n",
       " (366, 1),\n",
       " (376, 1),\n",
       " (394, 1),\n",
       " (396, 1),\n",
       " (401, 1),\n",
       " (421, 1),\n",
       " (424, 1),\n",
       " (435, 1),\n",
       " (455, 3),\n",
       " (456, 2)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_text]\n",
    "print (len(bow_corpus))\n",
    "bow_corpus[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"amazing\") appears 6 time.\n",
      "Word 3 (\"bike\") appears 1 time.\n",
      "Word 5 (\"came\") appears 1 time.\n",
      "Word 14 (\"entrance\") appears 1 time.\n",
      "Word 15 (\"forest\") appears 2 time.\n",
      "Word 16 (\"hour\") appears 1 time.\n",
      "Word 20 (\"main\") appears 4 time.\n",
      "Word 24 (\"path\") appears 2 time.\n",
      "Word 27 (\"place\") appears 3 time.\n",
      "Word 30 (\"quick\") appears 2 time.\n",
      "Word 35 (\"route\") appears 1 time.\n",
      "Word 42 (\"time\") appears 2 time.\n",
      "Word 48 (\"year\") appears 1 time.\n",
      "Word 54 (\"bridge\") appears 1 time.\n",
      "Word 56 (\"busy\") appears 2 time.\n",
      "Word 63 (\"definitely\") appears 1 time.\n",
      "Word 65 (\"enjoyed\") appears 1 time.\n",
      "Word 70 (\"level\") appears 1 time.\n",
      "Word 72 (\"long\") appears 2 time.\n",
      "Word 73 (\"loop\") appears 2 time.\n",
      "Word 79 (\"scenery\") appears 4 time.\n",
      "Word 80 (\"short\") appears 1 time.\n",
      "Word 81 (\"spend\") appears 1 time.\n",
      "Word 82 (\"spot\") appears 1 time.\n",
      "Word 92 (\"went\") appears 1 time.\n",
      "Word 95 (\"absolutely\") appears 1 time.\n",
      "Word 99 (\"family\") appears 1 time.\n",
      "Word 102 (\"friendly\") appears 1 time.\n",
      "Word 103 (\"gorgeous\") appears 1 time.\n",
      "Word 108 (\"section\") appears 1 time.\n",
      "Word 113 (\"wild\") appears 1 time.\n",
      "Word 114 (\"wildlife\") appears 1 time.\n",
      "Word 120 (\"kept\") appears 1 time.\n",
      "Word 123 (\"marked\") appears 3 time.\n",
      "Word 127 (\"overall\") appears 2 time.\n",
      "Word 130 (\"pretty\") appears 2 time.\n",
      "Word 132 (\"suggest\") appears 1 time.\n",
      "Word 133 (\"thing\") appears 1 time.\n",
      "Word 137 (\"beginning\") appears 1 time.\n",
      "Word 139 (\"best\") appears 1 time.\n",
      "Word 141 (\"biking\") appears 1 time.\n",
      "Word 149 (\"confusing\") appears 1 time.\n",
      "Word 154 (\"different\") appears 1 time.\n",
      "Word 158 (\"elevation\") appears 3 time.\n",
      "Word 164 (\"fairly\") appears 1 time.\n",
      "Word 165 (\"fall\") appears 2 time.\n",
      "Word 171 (\"get\") appears 1 time.\n",
      "Word 172 (\"gravel\") appears 1 time.\n",
      "Word 173 (\"groomed\") appears 2 time.\n",
      "Word 178 (\"head\") appears 1 time.\n",
      "Word 180 (\"hill\") appears 2 time.\n",
      "Word 189 (\"looking\") appears 1 time.\n",
      "Word 191 (\"marker\") appears 1 time.\n",
      "Word 192 (\"minute\") appears 1 time.\n",
      "Word 195 (\"mountain\") appears 1 time.\n",
      "Word 203 (\"option\") appears 1 time.\n",
      "Word 206 (\"peaceful\") appears 1 time.\n",
      "Word 208 (\"photo\") appears 1 time.\n",
      "Word 213 (\"return\") appears 1 time.\n",
      "Word 215 (\"rocky\") appears 1 time.\n",
      "Word 223 (\"steep\") appears 1 time.\n",
      "Word 224 (\"stop\") appears 1 time.\n",
      "Word 225 (\"stream\") appears 2 time.\n",
      "Word 228 (\"terrain\") appears 1 time.\n",
      "Word 235 (\"tree\") appears 1 time.\n",
      "Word 238 (\"want\") appears 3 time.\n",
      "Word 241 (\"waterfall\") appears 5 time.\n",
      "Word 243 (\"weekend\") appears 1 time.\n",
      "Word 245 (\"winter\") appears 1 time.\n",
      "Word 247 (\"wonderful\") appears 1 time.\n",
      "Word 250 (\"young\") appears 1 time.\n",
      "Word 257 (\"change\") appears 1 time.\n",
      "Word 258 (\"choose\") appears 1 time.\n",
      "Word 259 (\"climb\") appears 1 time.\n",
      "Word 273 (\"favorite\") appears 1 time.\n",
      "Word 282 (\"incredible\") appears 1 time.\n",
      "Word 285 (\"lost\") appears 1 time.\n",
      "Word 286 (\"map\") appears 1 time.\n",
      "Word 287 (\"mile\") appears 1 time.\n",
      "Word 301 (\"sure\") appears 1 time.\n",
      "Word 302 (\"taken\") appears 1 time.\n",
      "Word 306 (\"variety\") appears 3 time.\n",
      "Word 326 (\"location\") appears 1 time.\n",
      "Word 329 (\"maybe\") appears 1 time.\n",
      "Word 332 (\"multiple\") appears 1 time.\n",
      "Word 336 (\"possible\") appears 1 time.\n",
      "Word 340 (\"seeing\") appears 2 time.\n",
      "Word 342 (\"sign\") appears 1 time.\n",
      "Word 344 (\"start\") appears 2 time.\n",
      "Word 353 (\"closer\") appears 2 time.\n",
      "Word 355 (\"dont\") appears 1 time.\n",
      "Word 360 (\"house\") appears 1 time.\n",
      "Word 385 (\"valley\") appears 1 time.\n",
      "Word 396 (\"easier\") appears 1 time.\n",
      "Word 397 (\"encountered\") appears 1 time.\n",
      "Word 398 (\"followed\") appears 1 time.\n",
      "Word 399 (\"green\") appears 1 time.\n",
      "Word 400 (\"horse\") appears 2 time.\n",
      "Word 401 (\"life\") appears 1 time.\n",
      "Word 402 (\"said\") appears 1 time.\n",
      "Word 403 (\"tall\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_400 = bow_corpus[10]\n",
    "for i in range(len(bow_doc_400)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_400[i][0], \n",
    "                                               dictionary[bow_doc_400[i][0]], \n",
    "bow_doc_400[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:06:06,372 : INFO : collecting document frequencies\n",
      "2019-01-27 15:06:06,373 : INFO : PROGRESS: processing document #0\n",
      "2019-01-27 15:06:06,383 : INFO : calculating IDF weights for 437 documents and 462 features (19028 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.12552508574677335),\n",
      " (1, 0.13003796433071327),\n",
      " (2, 0.13172016625443117),\n",
      " (3, 0.08711519099181415),\n",
      " (4, 0.11228304651859898),\n",
      " (5, 0.13346355240560367),\n",
      " (6, 0.15854693595334493),\n",
      " (7, 0.285554464969946),\n",
      " (8, 0.08385285525174707),\n",
      " (9, 0.1002355767458483),\n",
      " (10, 0.16042051065457558),\n",
      " (11, 0.17559034338713422),\n",
      " (12, 0.11343823098704314),\n",
      " (13, 0.25368184835436297),\n",
      " (14, 0.0936129485174261),\n",
      " (15, 0.1126211643699685),\n",
      " (16, 0.06155901935102995),\n",
      " (17, 0.15854693595334493),\n",
      " (18, 0.13715289815112755),\n",
      " (19, 0.06155901935102995),\n",
      " (20, 0.09599051853314206),\n",
      " (21, 0.15854693595334493),\n",
      " (22, 0.08385285525174707),\n",
      " (23, 0.0813939657327255),\n",
      " (24, 0.05321674516216979),\n",
      " (25, 0.3117747674741165),\n",
      " (26, 0.10204476638807146),\n",
      " (27, 0.08939818816045185),\n",
      " (28, 0.0656941535630294),\n",
      " (29, 0.12241274713339025),\n",
      " (30, 0.2223100907994733),\n",
      " (31, 0.1158356223119946),\n",
      " (32, 0.0790550708588436),\n",
      " (33, 0.17771698177978745),\n",
      " (34, 0.1411500320211086),\n",
      " (35, 0.09599051853314206),\n",
      " (36, 0.07574770192081645),\n",
      " (37, 0.11967269648168052),\n",
      " (38, 0.07166542089576206),\n",
      " (39, 0.1616407729761594),\n",
      " (40, 0.13003796433071327),\n",
      " (41, 0.11462194139248089),\n",
      " (42, 0.03605198616642563),\n",
      " (43, 0.10588184055775736),\n",
      " (44, 0.1616407729761594),\n",
      " (45, 0.12241274713339025),\n",
      " (46, 0.26692710481120735),\n",
      " (47, 0.1432809533119428),\n",
      " (48, 0.06612798309132338),\n",
      " (49, 0.1616407729761594),\n",
      " (50, 0.09284592678276676)]\n"
     ]
    }
   ],
   "source": [
    "#Bag of words\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:06:06,403 : INFO : using symmetric alpha at 0.1\n",
      "2019-01-27 15:06:06,404 : INFO : using symmetric eta at 0.1\n",
      "2019-01-27 15:06:06,405 : INFO : using serial LDA version on this node\n",
      "2019-01-27 15:06:06,408 : INFO : running online LDA training, 10 topics, 2 passes over the supplied corpus of 437 documents, updating every 4000 documents, evaluating every ~437 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-01-27 15:06:06,408 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2019-01-27 15:06:06,412 : INFO : training LDA model using 2 processes\n",
      "2019-01-27 15:06:06,437 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #437/437, outstanding queue size 1\n",
      "2019-01-27 15:06:08,265 : INFO : topic #5 (0.100): 0.014*\"time\" + 0.013*\"rock\" + 0.013*\"loop\" + 0.013*\"lake\" + 0.011*\"marked\" + 0.011*\"challenging\" + 0.011*\"amazing\" + 0.011*\"point\" + 0.010*\"place\" + 0.010*\"pretty\"\n",
      "2019-01-27 15:06:08,266 : INFO : topic #4 (0.100): 0.017*\"loop\" + 0.011*\"time\" + 0.011*\"little\" + 0.011*\"tree\" + 0.011*\"fall\" + 0.011*\"short\" + 0.011*\"lake\" + 0.010*\"forest\" + 0.010*\"marked\" + 0.010*\"road\"\n",
      "2019-01-27 15:06:08,267 : INFO : topic #1 (0.100): 0.015*\"lake\" + 0.012*\"little\" + 0.011*\"forest\" + 0.011*\"marked\" + 0.011*\"time\" + 0.011*\"path\" + 0.010*\"place\" + 0.010*\"dog\" + 0.010*\"hill\" + 0.010*\"bridge\"\n",
      "2019-01-27 15:06:08,268 : INFO : topic #7 (0.100): 0.019*\"time\" + 0.018*\"dog\" + 0.017*\"fall\" + 0.016*\"place\" + 0.013*\"section\" + 0.013*\"hour\" + 0.011*\"path\" + 0.010*\"family\" + 0.010*\"loop\" + 0.009*\"going\"\n",
      "2019-01-27 15:06:08,269 : INFO : topic #8 (0.100): 0.021*\"time\" + 0.012*\"lake\" + 0.011*\"water\" + 0.010*\"marked\" + 0.010*\"place\" + 0.010*\"spot\" + 0.009*\"little\" + 0.009*\"definitely\" + 0.009*\"lookout\" + 0.009*\"forest\"\n",
      "2019-01-27 15:06:08,272 : INFO : topic diff=0.904907, rho=1.000000\n",
      "2019-01-27 15:06:08,687 : INFO : -6.191 per-word bound, 73.1 perplexity estimate based on a held-out corpus of 437 documents with 26419 words\n",
      "2019-01-27 15:06:08,688 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #437/437, outstanding queue size 1\n",
      "2019-01-27 15:06:09,089 : INFO : topic #4 (0.100): 0.019*\"loop\" + 0.013*\"short\" + 0.012*\"little\" + 0.011*\"time\" + 0.011*\"forest\" + 0.010*\"tree\" + 0.010*\"fall\" + 0.010*\"place\" + 0.010*\"lake\" + 0.010*\"hour\"\n",
      "2019-01-27 15:06:09,090 : INFO : topic #5 (0.100): 0.015*\"rock\" + 0.015*\"loop\" + 0.014*\"lake\" + 0.014*\"time\" + 0.013*\"challenging\" + 0.013*\"point\" + 0.012*\"marked\" + 0.011*\"beach\" + 0.011*\"pretty\" + 0.011*\"amazing\"\n",
      "2019-01-27 15:06:09,091 : INFO : topic #0 (0.100): 0.032*\"lake\" + 0.023*\"place\" + 0.019*\"beach\" + 0.016*\"little\" + 0.015*\"dog\" + 0.012*\"definitely\" + 0.011*\"cliff\" + 0.010*\"long\" + 0.010*\"enjoy\" + 0.009*\"kid\"\n",
      "2019-01-27 15:06:09,092 : INFO : topic #6 (0.100): 0.026*\"lake\" + 0.016*\"time\" + 0.015*\"place\" + 0.014*\"moderate\" + 0.013*\"waterfall\" + 0.013*\"worth\" + 0.012*\"water\" + 0.012*\"rock\" + 0.011*\"spot\" + 0.011*\"definitely\"\n",
      "2019-01-27 15:06:09,094 : INFO : topic #1 (0.100): 0.015*\"lake\" + 0.014*\"dog\" + 0.014*\"little\" + 0.014*\"marked\" + 0.013*\"forest\" + 0.013*\"hill\" + 0.012*\"place\" + 0.012*\"path\" + 0.011*\"time\" + 0.011*\"bridge\"\n",
      "2019-01-27 15:06:09,095 : INFO : topic diff=0.489100, rho=0.671383\n",
      "2019-01-27 15:06:09,485 : INFO : -6.121 per-word bound, 69.6 perplexity estimate based on a held-out corpus of 437 documents with 26419 words\n",
      "2019-01-27 15:06:09,620 : INFO : topic #0 (0.100): 0.032*\"lake\" + 0.023*\"place\" + 0.019*\"beach\" + 0.016*\"little\" + 0.015*\"dog\" + 0.012*\"definitely\" + 0.011*\"cliff\" + 0.010*\"long\" + 0.010*\"enjoy\" + 0.009*\"kid\"\n",
      "2019-01-27 15:06:09,623 : INFO : topic #1 (0.100): 0.015*\"lake\" + 0.014*\"dog\" + 0.014*\"little\" + 0.014*\"marked\" + 0.013*\"forest\" + 0.013*\"hill\" + 0.012*\"place\" + 0.012*\"path\" + 0.011*\"time\" + 0.011*\"bridge\"\n",
      "2019-01-27 15:06:09,624 : INFO : topic #2 (0.100): 0.017*\"hill\" + 0.017*\"forest\" + 0.016*\"loop\" + 0.014*\"road\" + 0.014*\"dog\" + 0.014*\"path\" + 0.013*\"time\" + 0.013*\"section\" + 0.012*\"river\" + 0.012*\"little\"\n",
      "2019-01-27 15:06:09,628 : INFO : topic #3 (0.100): 0.017*\"time\" + 0.017*\"river\" + 0.013*\"road\" + 0.012*\"rock\" + 0.011*\"bike\" + 0.011*\"little\" + 0.011*\"fall\" + 0.010*\"lake\" + 0.010*\"path\" + 0.009*\"hour\"\n",
      "2019-01-27 15:06:09,629 : INFO : topic #4 (0.100): 0.019*\"loop\" + 0.013*\"short\" + 0.012*\"little\" + 0.011*\"time\" + 0.011*\"forest\" + 0.010*\"tree\" + 0.010*\"fall\" + 0.010*\"place\" + 0.010*\"lake\" + 0.010*\"hour\"\n",
      "2019-01-27 15:06:09,631 : INFO : topic #5 (0.100): 0.015*\"rock\" + 0.015*\"loop\" + 0.014*\"lake\" + 0.014*\"time\" + 0.013*\"challenging\" + 0.013*\"point\" + 0.012*\"marked\" + 0.011*\"beach\" + 0.011*\"pretty\" + 0.011*\"amazing\"\n",
      "2019-01-27 15:06:09,635 : INFO : topic #6 (0.100): 0.026*\"lake\" + 0.016*\"time\" + 0.015*\"place\" + 0.014*\"moderate\" + 0.013*\"waterfall\" + 0.013*\"worth\" + 0.012*\"water\" + 0.012*\"rock\" + 0.011*\"spot\" + 0.011*\"definitely\"\n",
      "2019-01-27 15:06:09,640 : INFO : topic #7 (0.100): 0.029*\"dog\" + 0.020*\"time\" + 0.018*\"place\" + 0.016*\"family\" + 0.014*\"section\" + 0.014*\"board\" + 0.012*\"hour\" + 0.011*\"path\" + 0.011*\"conservation\" + 0.010*\"come\"\n",
      "2019-01-27 15:06:09,642 : INFO : topic #8 (0.100): 0.022*\"time\" + 0.014*\"lake\" + 0.013*\"water\" + 0.012*\"lookout\" + 0.011*\"marked\" + 0.010*\"forest\" + 0.010*\"creek\" + 0.009*\"place\" + 0.009*\"spot\" + 0.009*\"year\"\n",
      "2019-01-27 15:06:09,644 : INFO : topic #9 (0.100): 0.047*\"fall\" + 0.019*\"little\" + 0.016*\"time\" + 0.012*\"definitely\" + 0.012*\"loop\" + 0.011*\"took\" + 0.011*\"waterfall\" + 0.011*\"challenging\" + 0.010*\"place\" + 0.010*\"bruce\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.032*\"lake\" + 0.023*\"place\" + 0.019*\"beach\" + 0.016*\"little\" + 0.015*\"dog\" + 0.012*\"definitely\" + 0.011*\"cliff\" + 0.010*\"long\" + 0.010*\"enjoy\" + 0.009*\"kid\"\n",
      "Topic: 1 \n",
      "Words: 0.015*\"lake\" + 0.014*\"dog\" + 0.014*\"little\" + 0.014*\"marked\" + 0.013*\"forest\" + 0.013*\"hill\" + 0.012*\"place\" + 0.012*\"path\" + 0.011*\"time\" + 0.011*\"bridge\"\n",
      "Topic: 2 \n",
      "Words: 0.017*\"hill\" + 0.017*\"forest\" + 0.016*\"loop\" + 0.014*\"road\" + 0.014*\"dog\" + 0.014*\"path\" + 0.013*\"time\" + 0.013*\"section\" + 0.012*\"river\" + 0.012*\"little\"\n",
      "Topic: 3 \n",
      "Words: 0.017*\"time\" + 0.017*\"river\" + 0.013*\"road\" + 0.012*\"rock\" + 0.011*\"bike\" + 0.011*\"little\" + 0.011*\"fall\" + 0.010*\"lake\" + 0.010*\"path\" + 0.009*\"hour\"\n",
      "Topic: 4 \n",
      "Words: 0.019*\"loop\" + 0.013*\"short\" + 0.012*\"little\" + 0.011*\"time\" + 0.011*\"forest\" + 0.010*\"tree\" + 0.010*\"fall\" + 0.010*\"place\" + 0.010*\"lake\" + 0.010*\"hour\"\n",
      "Topic: 5 \n",
      "Words: 0.015*\"rock\" + 0.015*\"loop\" + 0.014*\"lake\" + 0.014*\"time\" + 0.013*\"challenging\" + 0.013*\"point\" + 0.012*\"marked\" + 0.011*\"beach\" + 0.011*\"pretty\" + 0.011*\"amazing\"\n",
      "Topic: 6 \n",
      "Words: 0.026*\"lake\" + 0.016*\"time\" + 0.015*\"place\" + 0.014*\"moderate\" + 0.013*\"waterfall\" + 0.013*\"worth\" + 0.012*\"water\" + 0.012*\"rock\" + 0.011*\"spot\" + 0.011*\"definitely\"\n",
      "Topic: 7 \n",
      "Words: 0.029*\"dog\" + 0.020*\"time\" + 0.018*\"place\" + 0.016*\"family\" + 0.014*\"section\" + 0.014*\"board\" + 0.012*\"hour\" + 0.011*\"path\" + 0.011*\"conservation\" + 0.010*\"come\"\n",
      "Topic: 8 \n",
      "Words: 0.022*\"time\" + 0.014*\"lake\" + 0.013*\"water\" + 0.012*\"lookout\" + 0.011*\"marked\" + 0.010*\"forest\" + 0.010*\"creek\" + 0.009*\"place\" + 0.009*\"spot\" + 0.009*\"year\"\n",
      "Topic: 9 \n",
      "Words: 0.047*\"fall\" + 0.019*\"little\" + 0.016*\"time\" + 0.012*\"definitely\" + 0.012*\"loop\" + 0.011*\"took\" + 0.011*\"waterfall\" + 0.011*\"challenging\" + 0.010*\"place\" + 0.010*\"bruce\"\n"
     ]
    }
   ],
   "source": [
    "#Train lda model using gensim.models.LdaMulticore and save it to â€˜lda_modelâ€™\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "#Explore words in topic and relative weight\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-27 15:06:09,665 : INFO : using symmetric alpha at 0.1\n",
      "2019-01-27 15:06:09,670 : INFO : using symmetric eta at 0.1\n",
      "2019-01-27 15:06:09,672 : INFO : using serial LDA version on this node\n",
      "2019-01-27 15:06:09,676 : INFO : running online LDA training, 10 topics, 2 passes over the supplied corpus of 437 documents, updating every 8000 documents, evaluating every ~437 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-01-27 15:06:09,680 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2019-01-27 15:06:09,684 : INFO : training LDA model using 4 processes\n",
      "2019-01-27 15:06:10,241 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #437/437, outstanding queue size 1\n",
      "2019-01-27 15:06:12,645 : INFO : topic #1 (0.100): 0.008*\"river\" + 0.008*\"amazing\" + 0.008*\"creek\" + 0.007*\"beach\" + 0.006*\"marked\" + 0.006*\"wildlife\" + 0.006*\"point\" + 0.006*\"ning\" + 0.006*\"place\" + 0.006*\"lake\"\n",
      "2019-01-27 15:06:12,646 : INFO : topic #2 (0.100): 0.008*\"lake\" + 0.007*\"rock\" + 0.007*\"rocky\" + 0.007*\"kid\" + 0.006*\"lookout\" + 0.006*\"bruce\" + 0.006*\"ontario\" + 0.006*\"deer\" + 0.006*\"fall\" + 0.006*\"wife\"\n",
      "2019-01-27 15:06:12,647 : INFO : topic #6 (0.100): 0.008*\"lake\" + 0.007*\"busy\" + 0.007*\"place\" + 0.006*\"dog\" + 0.006*\"went\" + 0.006*\"moderate\" + 0.006*\"visit\" + 0.006*\"time\" + 0.006*\"pretty\" + 0.006*\"little\"\n",
      "2019-01-27 15:06:12,648 : INFO : topic #8 (0.100): 0.010*\"creek\" + 0.010*\"wood\" + 0.008*\"road\" + 0.008*\"beginner\" + 0.008*\"option\" + 0.007*\"muddy\" + 0.007*\"dog\" + 0.007*\"forest\" + 0.007*\"tree\" + 0.006*\"different\"\n",
      "2019-01-27 15:06:12,649 : INFO : topic #7 (0.100): 0.010*\"fall\" + 0.008*\"lake\" + 0.007*\"dog\" + 0.007*\"different\" + 0.007*\"water\" + 0.006*\"snow\" + 0.006*\"little\" + 0.006*\"people\" + 0.006*\"place\" + 0.006*\"scenery\"\n",
      "2019-01-27 15:06:12,651 : INFO : topic diff=1.956260, rho=1.000000\n",
      "2019-01-27 15:06:12,942 : INFO : -8.716 per-word bound, 420.4 perplexity estimate based on a held-out corpus of 437 documents with 2341 words\n",
      "2019-01-27 15:06:13,054 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #437/437, outstanding queue size 1\n",
      "2019-01-27 15:06:13,397 : INFO : topic #1 (0.100): 0.009*\"creek\" + 0.009*\"amazing\" + 0.009*\"direction\" + 0.008*\"river\" + 0.008*\"wildlife\" + 0.007*\"point\" + 0.007*\"marked\" + 0.007*\"beach\" + 0.006*\"lake\" + 0.006*\"house\"\n",
      "2019-01-27 15:06:13,399 : INFO : topic #6 (0.100): 0.009*\"lake\" + 0.008*\"busy\" + 0.008*\"watch\" + 0.008*\"went\" + 0.007*\"dog\" + 0.007*\"leash\" + 0.007*\"place\" + 0.007*\"visit\" + 0.006*\"city\" + 0.006*\"path\"\n",
      "2019-01-27 15:06:13,401 : INFO : topic #5 (0.100): 0.008*\"maintained\" + 0.008*\"bike\" + 0.008*\"wooded\" + 0.008*\"river\" + 0.008*\"perfect\" + 0.008*\"place\" + 0.008*\"lake\" + 0.006*\"dog\" + 0.006*\"scenic\" + 0.006*\"lead\"\n",
      "2019-01-27 15:06:13,401 : INFO : topic #3 (0.100): 0.011*\"paved\" + 0.010*\"creek\" + 0.007*\"short\" + 0.007*\"forest\" + 0.007*\"fall\" + 0.007*\"muddy\" + 0.007*\"hill\" + 0.007*\"city\" + 0.006*\"road\" + 0.006*\"marking\"\n",
      "2019-01-27 15:06:13,402 : INFO : topic #8 (0.100): 0.013*\"wood\" + 0.011*\"creek\" + 0.010*\"beginner\" + 0.009*\"option\" + 0.007*\"short\" + 0.007*\"leaf\" + 0.007*\"tree\" + 0.007*\"beaver\" + 0.007*\"dog\" + 0.007*\"muddy\"\n",
      "2019-01-27 15:06:13,403 : INFO : topic diff=0.789073, rho=0.671383\n",
      "2019-01-27 15:06:13,673 : INFO : -8.346 per-word bound, 325.4 perplexity estimate based on a held-out corpus of 437 documents with 2341 words\n",
      "2019-01-27 15:06:13,767 : INFO : topic #0 (0.100): 0.010*\"fall\" + 0.010*\"lake\" + 0.008*\"river\" + 0.008*\"ride\" + 0.008*\"bruce\" + 0.007*\"short\" + 0.007*\"rock\" + 0.007*\"stair\" + 0.006*\"water\" + 0.006*\"waterfall\"\n",
      "2019-01-27 15:06:13,768 : INFO : topic #1 (0.100): 0.009*\"creek\" + 0.009*\"amazing\" + 0.009*\"direction\" + 0.008*\"river\" + 0.008*\"wildlife\" + 0.007*\"point\" + 0.007*\"marked\" + 0.007*\"beach\" + 0.006*\"lake\" + 0.006*\"house\"\n",
      "2019-01-27 15:06:13,769 : INFO : topic #2 (0.100): 0.008*\"wife\" + 0.008*\"rocky\" + 0.008*\"kid\" + 0.008*\"lake\" + 0.007*\"daughter\" + 0.007*\"deer\" + 0.007*\"ontario\" + 0.007*\"turned\" + 0.007*\"plenty\" + 0.006*\"pine\"\n",
      "2019-01-27 15:06:13,769 : INFO : topic #3 (0.100): 0.011*\"paved\" + 0.010*\"creek\" + 0.007*\"short\" + 0.007*\"forest\" + 0.007*\"fall\" + 0.007*\"muddy\" + 0.007*\"hill\" + 0.007*\"city\" + 0.006*\"road\" + 0.006*\"marking\"\n",
      "2019-01-27 15:06:13,770 : INFO : topic #4 (0.100): 0.007*\"fall\" + 0.007*\"river\" + 0.007*\"loop\" + 0.007*\"lake\" + 0.007*\"rock\" + 0.006*\"challenging\" + 0.006*\"hour\" + 0.006*\"path\" + 0.006*\"time\" + 0.006*\"marked\"\n",
      "2019-01-27 15:06:13,770 : INFO : topic #5 (0.100): 0.008*\"maintained\" + 0.008*\"bike\" + 0.008*\"wooded\" + 0.008*\"river\" + 0.008*\"perfect\" + 0.008*\"place\" + 0.008*\"lake\" + 0.006*\"dog\" + 0.006*\"scenic\" + 0.006*\"lead\"\n",
      "2019-01-27 15:06:13,772 : INFO : topic #6 (0.100): 0.009*\"lake\" + 0.008*\"busy\" + 0.008*\"watch\" + 0.008*\"went\" + 0.007*\"dog\" + 0.007*\"leash\" + 0.007*\"place\" + 0.007*\"visit\" + 0.006*\"city\" + 0.006*\"path\"\n",
      "2019-01-27 15:06:13,773 : INFO : topic #7 (0.100): 0.009*\"fall\" + 0.008*\"lake\" + 0.008*\"dog\" + 0.008*\"different\" + 0.007*\"variety\" + 0.007*\"little\" + 0.007*\"snow\" + 0.007*\"water\" + 0.007*\"today\" + 0.007*\"people\"\n",
      "2019-01-27 15:06:13,774 : INFO : topic #8 (0.100): 0.013*\"wood\" + 0.011*\"creek\" + 0.010*\"beginner\" + 0.009*\"option\" + 0.007*\"short\" + 0.007*\"leaf\" + 0.007*\"tree\" + 0.007*\"beaver\" + 0.007*\"dog\" + 0.007*\"muddy\"\n",
      "2019-01-27 15:06:13,776 : INFO : topic #9 (0.100): 0.014*\"cave\" + 0.009*\"town\" + 0.008*\"simple\" + 0.008*\"awesome\" + 0.008*\"river\" + 0.008*\"winter\" + 0.008*\"scenic\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.007*\"short\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.010*\"fall\" + 0.010*\"lake\" + 0.008*\"river\" + 0.008*\"ride\" + 0.008*\"bruce\" + 0.007*\"short\" + 0.007*\"rock\" + 0.007*\"stair\" + 0.006*\"water\" + 0.006*\"waterfall\"\n",
      "Topic: 1 Word: 0.009*\"creek\" + 0.009*\"amazing\" + 0.009*\"direction\" + 0.008*\"river\" + 0.008*\"wildlife\" + 0.007*\"point\" + 0.007*\"marked\" + 0.007*\"beach\" + 0.006*\"lake\" + 0.006*\"house\"\n",
      "Topic: 2 Word: 0.008*\"wife\" + 0.008*\"rocky\" + 0.008*\"kid\" + 0.008*\"lake\" + 0.007*\"daughter\" + 0.007*\"deer\" + 0.007*\"ontario\" + 0.007*\"turned\" + 0.007*\"plenty\" + 0.006*\"pine\"\n",
      "Topic: 3 Word: 0.011*\"paved\" + 0.010*\"creek\" + 0.007*\"short\" + 0.007*\"forest\" + 0.007*\"fall\" + 0.007*\"muddy\" + 0.007*\"hill\" + 0.007*\"city\" + 0.006*\"road\" + 0.006*\"marking\"\n",
      "Topic: 4 Word: 0.007*\"fall\" + 0.007*\"river\" + 0.007*\"loop\" + 0.007*\"lake\" + 0.007*\"rock\" + 0.006*\"challenging\" + 0.006*\"hour\" + 0.006*\"path\" + 0.006*\"time\" + 0.006*\"marked\"\n",
      "Topic: 5 Word: 0.008*\"maintained\" + 0.008*\"bike\" + 0.008*\"wooded\" + 0.008*\"river\" + 0.008*\"perfect\" + 0.008*\"place\" + 0.008*\"lake\" + 0.006*\"dog\" + 0.006*\"scenic\" + 0.006*\"lead\"\n",
      "Topic: 6 Word: 0.009*\"lake\" + 0.008*\"busy\" + 0.008*\"watch\" + 0.008*\"went\" + 0.007*\"dog\" + 0.007*\"leash\" + 0.007*\"place\" + 0.007*\"visit\" + 0.006*\"city\" + 0.006*\"path\"\n",
      "Topic: 7 Word: 0.009*\"fall\" + 0.008*\"lake\" + 0.008*\"dog\" + 0.008*\"different\" + 0.007*\"variety\" + 0.007*\"little\" + 0.007*\"snow\" + 0.007*\"water\" + 0.007*\"today\" + 0.007*\"people\"\n",
      "Topic: 8 Word: 0.013*\"wood\" + 0.011*\"creek\" + 0.010*\"beginner\" + 0.009*\"option\" + 0.007*\"short\" + 0.007*\"leaf\" + 0.007*\"tree\" + 0.007*\"beaver\" + 0.007*\"dog\" + 0.007*\"muddy\"\n",
      "Topic: 9 Word: 0.014*\"cave\" + 0.009*\"town\" + 0.008*\"simple\" + 0.008*\"awesome\" + 0.008*\"river\" + 0.008*\"winter\" + 0.008*\"scenic\" + 0.007*\"loop\" + 0.007*\"rock\" + 0.007*\"short\"\n"
     ]
    }
   ],
   "source": [
    "#LDA using TF-IDF\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part', 'paved', 'time', 'road', 'closed', 'couple', 'place', 'forcing', 'road', 'probably', 'route', 'bike', 'bikinga', 'paved', 'take', 'wooded', 'spent', 'hour', 'came', 'couple', 'deer', 'beaver', 'bunny', 'tiniest', 'squirrel', 'seen', 'chipmunk', 'bench', 'scenic', 'break', 'pit', 'bbqs', 'splash', 'jungle', 'near', 'main', 'lived', 'close', 'year', 'idea', 'place', 'pointed', 'outstarted', 'west', 'entrance', 'east', 'point', 'quick', 'small', 'forest', 'lakeshore', 'picnic', 'soft', 'mulcha', 'beautiful', 'traverse', 'beautiful', 'wooded', 'paved', 'path', 'wouldnt', 'know', 'youre', 'city', 'suddenly', 'lake', 'appears', 'view', 'beautiful', 'vista', 'ride', 'lakeshore', 'round', 'trip', 'starting', 'highland', 'creek', 'rode', 'east', 'itdid', 'quick', 'yesterday', 'amazing', 'wilderness', 'city', 'sung', 'skirted', 'tennis', 'court', 'built', 'campus', 'panam', 'game', 'amazing', 'growth', 'forest', 'wetland', 'highland', 'creek', 'choice', 'leaving', 'city', 'right', 'kingston', 'road', 'miss']\n",
      "\n",
      "Score: 0.8120218515396118\t \n",
      "Topic: 0.017*\"hill\" + 0.017*\"forest\" + 0.016*\"loop\" + 0.014*\"road\" + 0.014*\"dog\" + 0.014*\"path\" + 0.013*\"time\" + 0.013*\"section\" + 0.012*\"river\" + 0.012*\"little\"\n",
      "\n",
      "Score: 0.17585311830043793\t \n",
      "Topic: 0.019*\"loop\" + 0.013*\"short\" + 0.012*\"little\" + 0.011*\"time\" + 0.011*\"forest\" + 0.010*\"tree\" + 0.010*\"fall\" + 0.010*\"place\" + 0.010*\"lake\" + 0.010*\"hour\"\n"
     ]
    }
   ],
   "source": [
    "#Performance evaluation-LDA bag of words\n",
    "n=0\n",
    "print (processed_text[n])\n",
    "for index, score in sorted(lda_model[bow_corpus[n]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moderate', 'worth', 'coming', 'year', 'round', 'beautiful', 'spot', 'lunch', 'picnic', 'covered', 'need', 'icers', 'worry', 'free', 'ethis', 'wonderful', 'stroller', 'wheelchairthere', 'meter', 'soft', 'surface', 'maybe', 'hard', 'stroller', 'wheel', 'chair', 'remaining', 'maintained', 'nice', 'viewsthis', 'definitely', 'categorized', 'moderate', 'wheel', 'chair', 'accessible', 'fact', 'looking', 'accessible', 'nice', 'base', 'review', 'out-and-back', 'loop', 'nowwhile', 'long', 'personally', 'think', 'family', 'young', 'kid', 'easily', 'stroller', 'auto', 'traffic', 'loop', 'nicely', 'maintained', 'board', 'gravel', 'nice', 'view', 'lakenice', 'place', 'biking', 'beautiful', 'view', 'wellnot', 'crowed', 'space', 'lakerecently', 'ventured', 'beautiful', 'scenery', 'ended', 'length', 'board', 'make', 'island', 'lake', 'conversation', 'lake', 'large', 'take', 'lake', 'bridge', 'cross', 'different', 'part', 'lake', 'hill', 'lake', 'pretty', 'flat', 'bench', 'enjoy', 'view', 'small', 'patch', 'forest', 'taken', 'care', 'huronontario', 'give', 'free', 'entrance', 'definitely', 'coming', 'timesbeautiful', 'maintained', 'amazing', 'view', 'eastern', 'quickly', 'forget', 'orangeville', 'feel', 'cottage', 'countryvisited', 'july', 'marked', 'varied', 'scenery', 'water', 'viewsamazing', 'groomed', 'kept', 'hour', 'loop', 'hilly', 'moderate', 'intensity', 'mixed', 'terrain', 'well-kept', 'path', 'stroller', 'ners', 'bike', 'beautiful', 'lake', 'path', 'forest', 'path', 'bridge', 'water', 'point', 'check', 'memorial', 'garden', 'loopthis', 'beautiful', 'place', 'age', 'lakeside', 'loop', 'packed', 'luck', 'snow', 'covered', 'slippery', 'saturday', 'washroom', 'close', 'main', 'open', 'skating', 'fishing', 'curling', 'closed', 'loop', 'missing', 'recording', 'accurate', 'mapawesome']\n",
      "\n",
      "Score: 0.9674081206321716\t \n",
      "Topic: 0.007*\"fall\" + 0.007*\"river\" + 0.007*\"loop\" + 0.007*\"lake\" + 0.007*\"rock\" + 0.006*\"challenging\" + 0.006*\"hour\" + 0.006*\"path\" + 0.006*\"time\" + 0.006*\"marked\"\n",
      "\n",
      "Score: 0.024896953254938126\t \n",
      "Topic: 0.009*\"fall\" + 0.008*\"lake\" + 0.008*\"dog\" + 0.008*\"different\" + 0.007*\"variety\" + 0.007*\"little\" + 0.007*\"snow\" + 0.007*\"water\" + 0.007*\"today\" + 0.007*\"people\"\n"
     ]
    }
   ],
   "source": [
    "#Performance evaluation-LDA TF-IDF\n",
    "print (processed_text[40])\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[40]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run word2vec model and then save it\n",
    "# texts_stemmed = filter(None, [next_text.strip(' ').split(' ') for next_text in df_work['processed_text'][0]])\n",
    "# w2vmodel_stemmed = gensim.models.Word2Vec(texts_stemmed, size=100, window=5, min_count=5, workers=4)\n",
    "# w2vmodel_stemmed.save(savefolder+'w2v_stemmed_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
